{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='orange'> Keywords </font>\n",
    "\n",
    "- Preprocessing text data into useful representations\n",
    "- Working with recurrent neural networks\n",
    "- Using 1D convnets for sequence processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'orange'> Two fundamental deep-learning algorithms for sequence processing </font>\n",
    "\n",
    "- RNN\n",
    "- 1D convnets : one-dimensional version of the 2D convnets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='orange'> working with text data </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Data**\n",
    "\n",
    "- Text is one of the most widespread forms of sequence data.\n",
    "- It can be understood as either a sequence of characters or a sequence of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach**\n",
    "\n",
    "- Deep-learning for natural-language processing is pattern recognition applied to words, sentences, and paragraphs, in much the same way that computer vision is pattern recognition appied to pixels.\n",
    "- Like all other neural network, deep-learning models don't take as input raw text:\n",
    "    they only work with numeric tensors. vectorizing text is the process of transforming text into numeric tensors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Token, tokenization**\n",
    "\n",
    "- <font color ='red'> [words] Segment text into words, and transform each word into a vector </font>\n",
    "- [characters] Segment text into characters, and transform each characters, and a vector.\n",
    "- [n-grams] Extract n-grams of words or characters, and transform each n-gram into a vector. N-grams are overlapping groups of multiple consecutive words or characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From Token to (numeric) Vector**\n",
    "\n",
    "Tokens to  one-hot encoding or <font color='red'> token embedding (word embedding) </font>**\n",
    "\n",
    "- one-hot encoding (one-hot vector)\n",
    "\n",
    "    - sparse\n",
    "    - high dimensional\n",
    "    - manual encoding\n",
    "\n",
    "\n",
    "- word embedding\n",
    "\n",
    "    - dense\n",
    "    - low dimensional\n",
    "    - training from data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train - Neural Network**\n",
    "\n",
    "Using Embedding, we can start learn with neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> 1. Token </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['I love my dog',\n",
    "          'I love my cat']\n",
    "\n",
    "tokenizer = Tokenizer() # Tokenizer객체 생성, num_words인자를 이용해 빈도가 높은 순서대로 단어를 토큰화 할 수 있다. \n",
    "\n",
    "tokenizer.fit_on_texts(samples) # 단어 인덱스\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(samples) #문자열을 정수 인덱스의 리스트로 변환한다.\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x7f4a59225f90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4], [1, 2, 3, 5]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Word Indexing \n",
    "\n",
    "\n",
    "- the word for example \"listen\" is represented by number using an encoding scheme (ASCII CODE... etc..)\n",
    "- But the order of number is important\n",
    "\n",
    "`listen = [76, 73, 83, 84, 69, 78]`\n",
    "\n",
    "`silent = [83, 73, 76, 69, 78, 84]`\n",
    "\n",
    "- this bunch of numbers can then represent the word listen but word silent has the same letters, and thus the same numbers, just in a different order.\n",
    "- It makes it hard for us to understand sentiment of a word just by the letters in it\n",
    "\n",
    "- So it might be easier, instead of encoding letters to encode words.\n",
    "- Consider the sentence I love my dog.\n",
    "- what would happen if we start encoding the words in this sentence instead of letters in each word?\n",
    "\n",
    "`sentence1 = {'I':1, \"love\":2, \"my\":3, \"dog\":4}`\n",
    "\n",
    "`sentence2 = {'I':1, \"love\":2, \"my\":3, \"cat\":5}`\n",
    "\n",
    "- two sentences are already show some form of similarity between them.\n",
    "- And it's a similarity you would expect, because they're both about loving a pet.\n",
    "- Given this method of encoding sentences into number, now let's kate a look at some code to archieve this for us. : This process, is called tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Sequencing - Turning sentences into data\n",
    "\n",
    "- creating sequences of numbers from your sentences\n",
    "- And using tools to process them to make them ready for teaching neural network\n",
    "- Last time, we saw how to take a set of sentences and use the tokenizer to turn the words into numberic tokens.\n",
    "- Let's build on that now by also seeing how the senteces containing these words.\n",
    "- Can be turned into sequences of numbers.\n",
    "- We'll add another sentence to our set of texts, and I'm doing this because the existing sentences all have four words\n",
    "- and it's important to see how to manage sentences, or sequences, of different lengths\n",
    "- The tokenizer supports a method called texts to sequences which performs most of the work for you.\n",
    "- It creates sequences of tokens representing each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'my': 1,\n",
       " 'love': 2,\n",
       " 'dog': 3,\n",
       " 'i': 4,\n",
       " 'you': 5,\n",
       " 'cat': 6,\n",
       " 'do': 7,\n",
       " 'think': 8,\n",
       " 'is': 9,\n",
       " 'amazing': 10}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = ['I love my dog',\n",
    "            'I love my cat',\n",
    "            'You love my dog!',\n",
    "            'Do you think my dog is amazing?']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100)\n",
    "tokenizer.fit_on_texts(sentence)\n",
    "word_index = tokenizer.word_index\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Basic Tokenization Done!\n",
    "\n",
    "\n",
    "- this is all very well for getting data ready for training a neural network,\n",
    "- but what happens when that neural network needs to classify texts, but there are words in the text that it has never seen before?\n",
    "- This can confuse the tokenizer, so we'll look at how to handle that next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) OOV 처리, (Out of Vocabrary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 2, 1, 3], [1, 3, 1]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = ['i really love my dog',\n",
    "             'my dog loves my manatee']\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "test_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'my': 2,\n",
       " 'love': 3,\n",
       " 'dog': 4,\n",
       " 'i': 5,\n",
       " 'you': 6,\n",
       " 'cat': 7,\n",
       " 'do': 8,\n",
       " 'think': 9,\n",
       " 'is': 10,\n",
       " 'amazing': 11}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentence)\n",
    "word_index = tokenizer.word_index\n",
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "test_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  4,  2,  1,  3],\n",
       "       [ 0,  0,  0,  4,  2,  1,  6],\n",
       "       [ 0,  0,  0,  5,  2,  1,  3],\n",
       "       [ 7,  5,  8,  1,  3,  9, 10]], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded = pad_sequences(sequences)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  2,  1,  3,  0,  0,  0],\n",
       "       [ 4,  2,  1,  6,  0,  0,  0],\n",
       "       [ 5,  2,  1,  3,  0,  0,  0],\n",
       "       [ 7,  5,  8,  1,  3,  9, 10]], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, padding='post')\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  2,  1,  3,  0],\n",
       "       [ 4,  2,  1,  6,  0],\n",
       "       [ 5,  2,  1,  3,  0],\n",
       "       [ 8,  1,  3,  9, 10]], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, padding='post', maxlen=5)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 2, 1, 3, 0],\n",
       "       [4, 2, 1, 6, 0],\n",
       "       [5, 2, 1, 3, 0],\n",
       "       [7, 5, 8, 1, 3]], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, padding='post', truncating='post', maxlen=5)\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> 2 Numeric vector </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) one-hot encoding (one-hot vector) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1., 0.],\n",
       "       [0., 1., 1., 1., 0., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "one_hot_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.embeddings.Embedding at 0x7f0b8bbb5450>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(1000, 64) # 최소 2개의 인자를 받는다, 토큰의 갯수, 임베딩의 차원\n",
    "embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>보통 Deep leaerning에는 word embedding을 많이 사용한다.!</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials/text/word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0058084   0.01899623  0.03751728  0.01505322 -0.0078648   0.02411301\n",
      "   0.0432946   0.03782305  0.0476147  -0.00416829  0.04019305 -0.04721996\n",
      "  -0.02458578 -0.03131541 -0.04732466  0.03429008 -0.00307468  0.01407934\n",
      "  -0.03312021  0.02726435 -0.0258376   0.01107335 -0.02803576  0.01268565\n",
      "   0.0117619   0.03730077 -0.00888462  0.04984557  0.0351714   0.02382283\n",
      "   0.04052938  0.00780154  0.02174998  0.00192178 -0.01092023 -0.04568827\n",
      "  -0.00639306 -0.00019554  0.02904104  0.02959052  0.03528127  0.01469796\n",
      "  -0.00973715  0.03643007  0.01550866  0.03681531  0.00551854  0.02855345\n",
      "   0.01155401  0.00609125  0.02270531 -0.00458912  0.02443899 -0.01230549\n",
      "   0.04663397 -0.00156844  0.03369036 -0.00010158 -0.03046843  0.02773331\n",
      "  -0.00327218 -0.01701242 -0.02675983  0.00358992]\n",
      " [ 0.02913376 -0.00439787  0.01085889  0.01547484 -0.03544807 -0.04792522\n",
      "   0.00144672 -0.04208839 -0.017596    0.03704337  0.00145243 -0.02046359\n",
      "   0.03297393 -0.0224851   0.01063949  0.00953243 -0.04315984  0.04056526\n",
      "  -0.036233   -0.01238266 -0.02503643  0.03034982  0.03783487 -0.00139878\n",
      "  -0.03192063  0.01259539  0.02541978 -0.02297052 -0.04246303  0.0182428\n",
      "   0.03664725 -0.0079282   0.02280033 -0.02682723 -0.02574766  0.00915293\n",
      "   0.00290801 -0.0256018   0.00042477 -0.02816875 -0.00948497  0.00550581\n",
      "   0.03043574  0.0260113   0.03452678  0.03178439  0.01681692 -0.03180299\n",
      "  -0.0430653  -0.02973781  0.01309773  0.03528095  0.00248761  0.0213214\n",
      "  -0.0322356   0.02752382  0.04234516 -0.04634457  0.0302652  -0.02939812\n",
      "  -0.03806199  0.04278897  0.0151295   0.03893728]\n",
      " [-0.00306666 -0.02724366  0.00043339 -0.03114032 -0.03451179 -0.03103359\n",
      "  -0.02083466  0.01948426  0.03162798 -0.01063831 -0.01084919 -0.02390105\n",
      "  -0.03293015 -0.01446053 -0.0080759   0.02482307 -0.03622103  0.0050393\n",
      "  -0.0243835  -0.01963834  0.03747623 -0.02238037  0.023662   -0.03898065\n",
      "  -0.03277198 -0.00504427 -0.04710317  0.04873525  0.0004984   0.04322637\n",
      "  -0.01016139  0.01302585  0.0136484   0.03156242  0.04571071 -0.00152382\n",
      "   0.02491676  0.03587211  0.02196852  0.01234152 -0.04155215 -0.02543234\n",
      "  -0.00121833  0.0067475   0.03895884 -0.00607533  0.0318395   0.03061323\n",
      "   0.02790094  0.02100221 -0.01246578 -0.04118445 -0.00856615 -0.01529824\n",
      "  -0.04420995  0.01099329 -0.01956257 -0.04597051  0.01132821 -0.01213081\n",
      "  -0.02409842  0.03377852  0.02577763  0.04543578]]\n",
      "(3, 64)\n",
      "2\n",
      "192\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "check = embedding_layer(tf.constant([1,2,3]))\n",
    "print(check.numpy())\n",
    "print(check.numpy().shape)\n",
    "print(check.numpy().ndim)\n",
    "print(check.numpy().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 64])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([[0,1,2],[3,4,5]]))\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> 3. Training - NN</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2  https://local.theonion.com/mom-starting-to-fea...   \n",
       "3  https://politics.theonion.com/boehner-just-wan...   \n",
       "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "\n",
       "                                            headline  is_sarcastic  \n",
       "0  former versace store clerk sues over secret 'b...             0  \n",
       "1  the 'roseanne' revival catches up to our thorn...             0  \n",
       "2  mom starting to fear son's web series closest ...             1  \n",
       "3  boehner just wants wife to listen, not come up...             1  \n",
       "4  j.k. rowling wishes snape happy birthday in th...             0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"./Sarcasm_Headlines_Dataset.json\", lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(df['article_link'])\n",
    "labels = list(df['is_sarcastic'])\n",
    "urls = list(df['article_link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Tokenization Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    2     4     5     3     6 12731    95  2105     8 12732     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0]\n",
      "(26709, 46)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequences, padding='post')\n",
    "\n",
    "print(padded[0])\n",
    "print(padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26709"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Tokenization Data - train / test data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = 20000\n",
    "training_sentences = sentences[0:training_size]\n",
    "testing_sentences = sentences[training_size:]\n",
    "training_labels = labels[0:training_size]\n",
    "testing_labels = labels[training_size:]\n",
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 100\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2    4    5    3    6    1   93 1840    8    1    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "(20000, 100)\n",
      "[   2    4    7    3    1  961 6935 6017   68    1 3031    1    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "(6709, 100)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "print(training_padded[0])\n",
    "print(training_padded.shape)\n",
    "print(testing_padded[0])\n",
    "print(testing_padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Build a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련을 시키기 위해선, 특성 행렬과 타겟 벡터 형태로!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6709, 100)\n",
      "(6709,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "training_padded = np.array(training_padded)\n",
    "training_labels = np.array(training_labels)\n",
    "testing_padded = np.array(testing_padded)\n",
    "testing_labels = np.array(testing_labels)\n",
    "print(testing_padded.shape)\n",
    "print(testing_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "625/625 - 1s - loss: 7.2134e-04 - accuracy: 1.0000 - val_loss: 7.9016e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/30\n",
      "625/625 - 1s - loss: 4.1782e-04 - accuracy: 1.0000 - val_loss: 5.0616e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/30\n",
      "625/625 - 1s - loss: 2.5761e-04 - accuracy: 1.0000 - val_loss: 3.4227e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/30\n",
      "625/625 - 1s - loss: 1.6447e-04 - accuracy: 1.0000 - val_loss: 2.3583e-04 - val_accuracy: 1.0000\n",
      "Epoch 5/30\n",
      "625/625 - 1s - loss: 1.0745e-04 - accuracy: 1.0000 - val_loss: 1.6677e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/30\n",
      "625/625 - 1s - loss: 7.1384e-05 - accuracy: 1.0000 - val_loss: 1.2029e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/30\n",
      "625/625 - 1s - loss: 4.7963e-05 - accuracy: 1.0000 - val_loss: 9.0381e-05 - val_accuracy: 1.0000\n",
      "Epoch 8/30\n",
      "625/625 - 1s - loss: 3.2607e-05 - accuracy: 1.0000 - val_loss: 7.0274e-05 - val_accuracy: 1.0000\n",
      "Epoch 9/30\n",
      "625/625 - 1s - loss: 2.2509e-05 - accuracy: 1.0000 - val_loss: 4.9447e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/30\n",
      "625/625 - 1s - loss: 1.5480e-05 - accuracy: 1.0000 - val_loss: 3.7034e-05 - val_accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "625/625 - 1s - loss: 1.0785e-05 - accuracy: 1.0000 - val_loss: 2.8874e-05 - val_accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "625/625 - 1s - loss: 7.4604e-06 - accuracy: 1.0000 - val_loss: 2.2095e-05 - val_accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "625/625 - 1s - loss: 5.2815e-06 - accuracy: 1.0000 - val_loss: 1.6975e-05 - val_accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "625/625 - 1s - loss: 3.6778e-06 - accuracy: 1.0000 - val_loss: 1.3016e-05 - val_accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "625/625 - 2s - loss: 2.6258e-06 - accuracy: 1.0000 - val_loss: 1.0181e-05 - val_accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "625/625 - 1s - loss: 1.8367e-06 - accuracy: 1.0000 - val_loss: 8.4345e-06 - val_accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "625/625 - 1s - loss: 1.3116e-06 - accuracy: 1.0000 - val_loss: 6.3251e-06 - val_accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "625/625 - 1s - loss: 9.3325e-07 - accuracy: 1.0000 - val_loss: 5.0790e-06 - val_accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "625/625 - 1s - loss: 6.6837e-07 - accuracy: 1.0000 - val_loss: 4.4810e-06 - val_accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "625/625 - 1s - loss: 4.8869e-07 - accuracy: 1.0000 - val_loss: 3.3347e-06 - val_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "625/625 - 1s - loss: 3.4861e-07 - accuracy: 1.0000 - val_loss: 2.8251e-06 - val_accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "625/625 - 1s - loss: 2.4948e-07 - accuracy: 1.0000 - val_loss: 2.2039e-06 - val_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "625/625 - 1s - loss: 1.8366e-07 - accuracy: 1.0000 - val_loss: 1.7541e-06 - val_accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "625/625 - 1s - loss: 1.3498e-07 - accuracy: 1.0000 - val_loss: 2.5912e-06 - val_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "625/625 - 1s - loss: 9.8580e-08 - accuracy: 1.0000 - val_loss: 1.2021e-06 - val_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "625/625 - 1s - loss: 7.4959e-08 - accuracy: 1.0000 - val_loss: 1.0219e-06 - val_accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "625/625 - 1s - loss: 5.7941e-08 - accuracy: 1.0000 - val_loss: 9.2115e-07 - val_accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "625/625 - 1s - loss: 4.2898e-08 - accuracy: 1.0000 - val_loss: 7.8522e-07 - val_accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "625/625 - 1s - loss: 3.4357e-08 - accuracy: 1.0000 - val_loss: 6.1254e-07 - val_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "625/625 - 1s - loss: 2.6749e-08 - accuracy: 1.0000 - val_loss: 5.3212e-07 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99607193]\n",
      " [0.9766431 ]]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"granny starting to fear spiders in the garden might be real\", \"game of thrones season finale showing this sunday night\"]\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "print(model.predict(padded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) load_data - tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-3.1.0-py3-none-any.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 905 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill\n",
      "  Downloading dill-0.3.2.zip (177 kB)\n",
      "\u001b[K     |████████████████████████████████| 177 kB 11.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=18.1.0 in /home/reisei88/anaconda3/lib/python3.7/site-packages (from tensorflow_datasets) (19.3.0)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: six in /home/reisei88/anaconda3/lib/python3.7/site-packages (from tensorflow_datasets) (1.14.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/reisei88/anaconda3/lib/python3.7/site-packages (from tensorflow_datasets) (2.22.0)\n",
      "Requirement already satisfied: wrapt in /home/reisei88/anaconda3/lib/python3.7/site-packages (from tensorflow_datasets) (1.11.2)\n",
      "Requirement already satisfied: numpy in /home/reisei88/anaconda3/lib/python3.7/site-packages (from tensorflow_datasets) (1.18.1)\n",
      "Requirement already satisfied: absl-py in /home/reisei88/anaconda3/lib/python3.7/site-packages (from tensorflow_datasets) (0.9.0)\n",
      "Requirement already satisfied: tqdm in /home/reisei88/anaconda3/lib/python3.7/site-packages (from tensorflow_datasets) (4.42.1)\n",
      "Requirement already satisfied: termcolor in /home/reisei88/anaconda3/lib/python3.7/site-packages (from tensorflow_datasets) (1.1.0)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-0.22.2-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/reisei88/anaconda3/lib/python3.7/site-packages (from tensorflow_datasets) (3.12.2)\n",
      "Requirement already satisfied: future in /home/reisei88/anaconda3/lib/python3.7/site-packages (from tensorflow_datasets) (0.18.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/reisei88/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/reisei88/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/reisei88/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/reisei88/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.25.8)\n",
      "Collecting googleapis-common-protos\n",
      "  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 9.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/reisei88/anaconda3/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow_datasets) (45.2.0.post20200210)\n",
      "Building wheels for collected packages: dill, promise\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.2-py3-none-any.whl size=78912 sha256=c15109f45a1cdd48afcb070fa623d891648a613009924b6e2058af92a6262e92\n",
      "  Stored in directory: /home/reisei88/.cache/pip/wheels/72/6b/d5/5548aa1b73b8c3d176ea13f9f92066b02e82141549d90e2100\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21495 sha256=75a88fe25db62f8edc659cd4c70cdb0c75207c7d32cadf2ff3473dbbe0f5e789\n",
      "  Stored in directory: /home/reisei88/.cache/pip/wheels/29/93/c6/762e359f8cb6a5b69c72235d798804cae523bbe41c2aa8333d\n",
      "Successfully built dill promise\n",
      "Installing collected packages: dill, promise, googleapis-common-protos, tensorflow-metadata, tensorflow-datasets\n",
      "Successfully installed dill-0.3.2 googleapis-common-protos-1.52.0 promise-2.3 tensorflow-datasets-3.1.0 tensorflow-metadata-0.22.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset imdb_reviews/subwords8k/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /home/reisei88/tensorflow_datasets/imdb_reviews/subwords8k/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46c8f75e75c4c9c8a8a2f1da696c85d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0dad5c7f6d94ec1a9e3b85798d390be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Shuffling and writing examples to /home/reisei88/tensorflow_datasets/imdb_reviews/subwords8k/1.0.0.incompleteIUF8HG/imdb_reviews-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f7571bcc8745c09d2ec340eee991ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Shuffling and writing examples to /home/reisei88/tensorflow_datasets/imdb_reviews/subwords8k/1.0.0.incompleteIUF8HG/imdb_reviews-test.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b18fb8f31554d138617e69699176e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Shuffling and writing examples to /home/reisei88/tensorflow_datasets/imdb_reviews/subwords8k/1.0.0.incompleteIUF8HG/imdb_reviews-unsupervised.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f49ce6b17a545f8b8f322e98490484e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to /home/reisei88/tensorflow_datasets/imdb_reviews/subwords8k/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
      "\r"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "imdb, info = tfds.load(\"imdb_reviews/subwords8k\", with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the_', ', ', '. ', 'a_', 'and_', 'of_', 'to_', 's_', 'is_', 'br', 'in_', 'I_', 'that_', 'this_', 'it_', ' /><', ' />', 'was_', 'The_', 'as_', 't_', 'with_', 'for_', '.<', 'on_', 'but_', 'movie_', ' (', 'are_', 'his_', 'have_', 'film_', 'not_', 'ing_', 'be_', 'ed_', 'you_', ' \"', 'it', 'd_', 'an_', 'he_', 'by_', 'at_', 'one_', 'who_', 'y_', 'from_', 'e_', 'or_', 'all_', 'like_', 'they_', '\" ', 'so_', 'just_', 'has_', ') ', 'her_', 'about_', 'out_', 'This_', 'some_', 'ly_', 'movie', 'film', 'very_', 'more_', 'It_', 'would_', 'what_', 'when_', 'which_', 'good_', 'if_', 'up_', 'only_', 'even_', 'their_', 'had_', 'really_', 'my_', 'can_', 'no_', 'were_', 'see_', 'she_', '? ', 'than_', '! ', 'there_', 'get_', 'been_', 'into_', ' - ', 'will_', 'much_', 'story_', 'because_', 'ing', 'time_', 'n_', 'we_', 'ed', 'me_', ': ', 'most_', 'other_', 'don', 'do_', 'm_', 'es_', 'how_', 'also_', 'make_', 'its_', 'could_', 'first_', 'any_', \"' \", 'people_', 'great_', 've_', 'ly', 'er_', 'made_', 'r_', 'But_', 'think_', \" '\", 'i_', 'bad_', 'A_', 'And_', 'It', 'on', '; ', 'him_', 'being_', 'never_', 'way_', 'that', 'many_', 'then_', 'where_', 'two_', 'In_', 'after_', 'too_', 'little_', 'you', '), ', 'well_', 'ng_', 'your_', 'If_', 'l_', '). ', 'does_', 'ever_', 'them_', 'did_', 'watch_', 'know_', 'seen_', 'time', 'er', 'character_', 'over_', 'characters_', 'movies_', 'man_', 'There_', 'love_', 'best_', 'still_', 'off_', 'such_', 'in', 'should_', 'the', 're_', 'He_', 'plot_', 'films_', 'go_', 'these_', 'acting_', 'doesn', 'es', 'show_', 'through_', 'better_', 'al_', 'something_', 'didn', 'back_', 'those_', 'us_', 'less_', '...', 'say_', 'is', 'one', 'makes_', 'and', 'can', 'all', 'ion_', 'find_', 'scene_', 'old_', 'real_', 'few_', 'going_', 'well', 'actually_', 'watching_', 'life_', 'me', '. <', 'o_', 'man', 'there', 'scenes_', 'same_', 'he', 'end_', 'this', '... ', 'k_', 'while_', 'thing_', 'of', 'look_', 'quite_', 'out', 'lot_', 'want_', 'why_', 'seems_', 'every_', 'll_', 'pretty_', 'got_', 'able_', 'nothing_', 'good', 'As_', 'story', ' & ', 'another_', 'take_', 'to', 'years_', 'between_', 'give_', 'am_', 'work_', 'isn', 'part_', 'before_', 'actors_', 'may_', 'gets_', 'young_', 'down_', 'around_', 'ng', 'thought_', 'though_', 'end', 'without_', 'What_', 'They_', 'things_', 'life', 'always_', 'must_', 'cast_', 'almost_', 'h_', '10', 'saw_', 'own_', 'here', 'bit_', 'come_', 'both_', 'might_', 'g_', 'whole_', 'new_', 'director_', 'them', 'horror_', 'ce', 'You_', 'least_', 'bad', 'big_', 'enough_', 'him', 'feel_', 'probably_', 'up', 'here_', 'making_', 'long_', 'her', 'st_', 'kind_', '--', 'original_', 'fact_', 'rather_', 'or', 'far_', 'nt_', 'played_', 'found_', 'last_', 'movies', 'When_', 'so', '\", ', 'comes_', 'action_', 'She_', 've', 'our_', 'anything_', 'funny_', 'ion', 'right_', 'way', 'trying_', 'now_', 'ous_', 'each_', 'done_', 'since_', 'ic_', 'point_', '\". ', 'wasn', 'interesting_', 'c_', 'worst_', 'te_', 'le_', 'ble_', 'ty_', 'looks_', 'show', 'put_', 'looking_', 'especially_', 'believe_', 'en_', 'goes_', 'over', 'ce_', 'p_', 'films', 'hard_', 'main_', 'be', 'having_', 'ry', 'TV_', 'worth_', 'One_', 'do', 'al', 're', 'again', 'series_', 'takes_', 'guy_', 'family_', 'seem_', 'plays_', 'role_', 'away_', 'world_', 'My_', 'character', ', \"', 'performance_', '2_', 'So_', 'watched_', 'John_', 'th_', 'plot', 'script_', 'For_', 'sure_', 'characters', 'set_', 'different_', 'minutes_', 'All_', 'American_', 'anyone_', 'Not_', 'music_', 'ry_', 'shows_', 'too', 'son_', 'en', 'day_', 'use_', 'someone_', 'for', 'woman_', 'yet_', '.\" ', 'during_', 'she', 'ro', '- ', 'times_', 'left_', 'used_', 'le', 'three_', 'play_', 'work', 'ness_', 'We_', 'girl_', 'comedy_', 'ment_', 'an', 'simply_', 'off', 'ies_', 'funny', 'ne', 'acting', 'That_', 'fun_', 'completely_', 'st', 'seeing_', 'us', 'te', 'special_', 'ation_', 'as', 'ive_', 'ful_', 'read_', 'reason_', 'co', 'need_', 'sa', 'true_', 'ted_', 'like', 'ck', 'place_', 'they', '10_', 'However', 'until_', 'rest_', 'sense_', 'ity_', 'everything_', 'people', 'nt', 'ending_', 'again_', 'ers_', 'given_', 'idea_', 'let_', 'nice_', 'help_', 'no', 'truly_', 'beautiful_', 'ter', 'ck_', 'version_', 'try_', 'came_', 'Even_', 'DVD_', 'se', 'mis', 'scene', 'job_', 'ting_', 'Me', 'At_', 'who', 'money_', 'ment', 'ch', 'recommend_', 'was', 'once_', 'getting_', 'tell_', 'de_', 'gives_', 'not', 'Lo', 'we', 'son', 'shot_', 'second_', 'After_', 'To_', 'high_', 'screen_', ' -- ', 'keep_', 'felt_', 'with', 'great', 'everyone_', 'although_', 'poor_', 'el', 'half_', 'playing_', 'couple_', 'now', 'ble', 'excellent_', 'enjoy_', 'couldn', 'x_', 'ne_', ',\" ', 'ie_', 'go', 'become_', 'less', 'himself_', 'supposed_', 'won', 'understand_', 'seen', 'ally_', 'THE_', 'se_', 'actor_', 'ts_', 'small_', 'line_', 'na', 'audience_', 'fan_', 'et', 'world', 'entire_', 'said_', 'at', '3_', 'scenes', 'rs_', 'full_', 'year_', 'men_', 'ke', 'doing_', 'went_', 'director', 'back', 'early_', 'Hollywood_', 'start_', 'liked_', 'against_', 'remember_', 'love', 'He', 'along_', 'ic', 'His_', 'wife_', 'effects_', 'together_', 'ch_', 'Ra', 'ty', 'maybe_', 'age', 'S_', 'While_', 'often_', 'sort_', 'definitely_', 'No', 'script', 'times', 'absolutely_', 'book_', 'day', 'human_', 'There', 'top_', 'ta', 'becomes_', 'piece_', 'waste_', 'seemed_', 'down', '5_', 'later_', 'rs', 'ja', 'certainly_', 'budget_', 'th', 'nce_', '200', '. (', 'age_', 'next_', 'ar', 'several_', 'ling_', 'short_', 'sh', 'fe', 'Of_', 'instead_', 'Man', 'T_', 'right', 'father_', 'actors', 'wanted_', 'cast', 'black_', 'Don', 'more', '1_', 'comedy', 'better', 'camera_', 'wonderful_', 'production_', 'inter', 'course', 'low_', 'else_', 'w_', 'ness', 'course_', 'based_', 'ti', 'Some_', 'know', 'house_', 'say', 'de', 'watch', 'ous', 'pro', 'tries_', 'ra', 'kids_', 'etc', ' \\x96 ', 'loved_', 'est_', 'fun', 'made', 'video_', 'un', 'totally_', 'Michael_', 'ho', 'mind_', 'No_', 'Be', 'ive', 'La', 'Fi', 'du', 'ers', 'Well', 'wants_', 'How_', 'series', 'performances_', 'written_', 'live_', 'New_', 'So', 'Ne', 'Na', 'night_', 'ge', 'gave_', 'home_', 'heart', 'women_', 'nu', 'ss_', 'hope_', 'ci', 'friends_', 'Se', 'years', 'sub', 'head_', 'Y_', 'Du', '. \"', 'turn_', 'red_', 'perfect_', 'already_', 'classic_', 'tri', 'ss', 'person_', 'star_', 'screen', 'style_', 'ur', 'starts_', 'under_', 'Then_', 'ke_', 'ine', 'ies', 'um', 'ie', 'face_', 'ir', 'enjoyed_', 'point', 'lines_', 'Mr', 'turns_', 'what', 'side_', 'sex_', 'Ha', 'final_', ').<', 'With_', 'care_', 'tion_', 'She', 'ation', 'Ar', 'ma', 'problem_', 'lost_', 'are', 'li', '4_', 'fully_', 'oo', 'sha', 'Just_', 'name_', 'ina', 'boy_', 'finally_', 'ol', '!<', 'Bo', 'about', 'though', 'hand', 'ton', 'lead_', 'school_', 'ns', 'ha', 'favorite_', 'stupid_', 'gi', 'original', 'mean_', 'To', 'took_', 'either_', 'ni', 'book', 'episode_', 'om', 'Su', 'D_', 'Mc', 'house', 'cannot_', 'stars_', 'behind_', 'see', 'other', 'Che', 'role', 'art', 'ever', 'Why_', 'father', 'case_', 'tic_', 'moments_', 'Co', 'works_', 'sound_', 'Ta', 'guess_', 'perhaps_', 'Vi', 'thing', 'fine_', 'fact', 'music', 'non', 'ful', 'action', 'ity', 'ct', 'ate_', 'type_', 'lack_', 'death_', 'art_', 'able', 'Ja', 'ge_', 'wouldn', 'am', 'tor', 'extremely_', 'pre', 'self', 'Mor', 'particularly_', 'bo', 'est', 'Ba', 'ya', 'play', 'Pa', 'ther', 'heard_', 'however', 'ver', 'dy_', 'Sa', 'ding_', 'led_', 'late_', 'feeling_', 'per', 'low', 'ably_', 'Un', 'On_', 'known_', 'kill_', 'fight_', 'beginning_', 'cat', 'bit', 'title_', 'vo', 'short', 'old', 'including_', 'Da', 'coming_', 'That', 'place', 'looked_', 'best', 'Lu', 'ent_', 'bla', 'quality_', 'except_', '...<', 'ff', 'decent_', 'much', 'De', 'Bu', 'ter_', 'attempt_', 'Bi', 'taking_', 'ig', 'Ti', 'whose_', 'dialogue_', 'zz', 'war_', 'ill', 'Te', 'war', 'Hu', 'James_', '..', 'under', 'ring_', 'pa', 'ot', 'expect_', 'Ga', 'itself_', 'line', 'lives_', 'let', 'Dr', 'mp', 'che', 'mean', 'called_', 'complete_', 'terrible_', 'boring_', 'others_', '\" (', 'aren', 'star', 'long', 'Li', 'mother_', 'si', 'highly_', 'ab', 'ex', 'os', 'nd', 'ten_', 'ten', 'run_', 'directed_', 'town_', 'friend_', 'David_', 'taken_', 'finds_', 'fans_', 'Mar', 'writing_', 'white_', 'u_', 'obviously_', 'mar', 'Ho', 'year', 'stop_', 'f_', 'leave_', 'king_', 'act_', 'mind', 'entertaining_', 'ish_', 'Ka', 'throughout_', 'viewer_', 'despite_', 'Robert_', 'somewhat_', 'hour_', 'car_', 'evil_', 'Although_', 'wrong_', 'Ro', 'dead_', 'body_', 'awful_', 'home', 'exactly_', 'bi', 'family', 'ts', 'usually_', 'told_', 'z_', 'oc', 'minutes', 'tra', 'some', 'actor', 'den', 'but', 'Sha', 'tu', 'strong_', 'Jo', 'real', 'la', 'gin', 'ul', 'amazing_', 'save_', 'wrong', 'dis', 'obvious_', 'close_', 'sometimes_', 'shown_', 'head', 'land', 'Go', 'mer', 'ending', 'else', 'audience', 'su', 'parts_', 'ga', 'before', 'cinema', 'opening_', 'laugh_', 'Ca', 'sh_', 'guys_', 'ds_', 'number_', 'Ma', 'soon_', 'ob', 'po', 'wonder_', 'group_', 'men', 'Mac', 'thinking_', 'fan', 'across_', 'turned_', 'ant', 'tells_', 'em', 'night', 'ton_', 'picture_', 'past_', 'Hi', 'girl', 'ght', 'woman', 'started_', 'ba', 'Ru', 'da', 'wi', 'running_', 'part', 'wish_', 'ner', 'ap', 'rn', 'ant_', 'mon', 'ast', 'awful', 'Yes', 'The', 'ard', 'nce', 'era', 'today', 'ad', 'Now_', '.) ', 'local_', 'killer_', 'huge_', 'flick', 'ends_', 'light', 'ons_', 'Al', 'knew_', 'due_', 'direction_', 'close', 'Gra', 'od', 'giving_', 'Le', 'op', 'Pe', 'ey_', 'wa', 'sta', 'worse_', 'single_', 'cut_', 'light_', 'ia', 'happens_', 'supporting_', 'room_', 'girls_', 'female_', 'E_', 'falls_', 'nd_', 'ish', 'mostly_', 'tan', 'major_', 'bring_', 'killed_', 'ele', 'el_', 'dark_', 'myself_', 'Pro', 'ent', 'ated_', 'British_', 'va', '....', 'talking_', 'con', 'tion', 'children_', 'by', 'voice_', 'sense', 'Car', '.. ', 'ain', 'For', 'Con', 'performance', 'au', 'stories_', 'ine_', 'Or', 'order_', 'first', 'ac', '8_', 'involved_', 'interesting', 'drama_', 'Dan', 'away', 'From_', 'ping_', 'boy', 'air', 'sing_', 'lle', 'You', 'lo', 'ian', 'ingly_', 'ia_', 'haven', 'using_', 'fo', 'dy', 'modern_', 'ST', 'wife', 'unt', 'game_', 'together', 'pp', 'clearly_', 'First_', 'sad', 'ris', 'ven', 'col', 'Maybe_', 'val', 'sexual_', 'serious_', 'relationship_', 'musical_', 'boring', 'But', 'hit_', 'brilliant_', 'easily_', 'living_', 'ca', 'police_', 'ip', ' , ', 'feels_', 'effects', 'sex', 'ist_', 'die', 'para', 'ort', 'humor_', 'Cor', 'ist', 'et_', 'Richard_', 'call_', 'example', 'appears_', 'actress_', 'rit', 'matter_', 'ar_', 'ns_', 'needs_', 'important_', 'fli', 'ec', 'stupid', 'ee', 'change_', 'bur', ' . ', 'comic_', 'DVD', 'We', '?<', 'Paul_', 'child_', 'ag', 'enjoy', 'cha', 'actual_', 'says_', 'nearly_', 'heart_', 'did', 'similar_', 'side', 'ru', 'ped_', 'und', 'super', 'name', 'clear_', \"', \", 'cu', 'child', 'moment_', 'ions_', 'fall_', 'done', 'chance_', 'then', 'ian_', 'George_', 'exc', 'enough', 'Jack_', 'win', 'Di', 'ying_', 'said', '80', 'ze', 'example_', 'themselves_', 'named_', 'ger', 'near_', 'guy', 'car', 'horrible_', 'bri', '!! ', 'ori', 'his', 'ded_', 'An_', 'released_', 'laugh', 'kept_', 'beyond_', 'b_', 'Sch', 'An', 'Lan', 'In', 'gar', 'genre', 'cho', 'Har', 'title', 'romantic_', 'mother', 'English_', 'mention_', 'interest_', 'Its_', 'money', 'face', 'brought_', 'ut', 'after', 'Win', 'working_', 'ny', 'knows_', 'happened_', 'certain_', '6_', 'within_', 'usual_', 'upon_', 'il', 'Her_', 'from', 'drama', 'Si', 'Mo', 'God', 'five_', 'whether_', 'tried_', 'ial_', 'history_', 'far', 'Re', 'novel', 'chi', 'inc', 'ure_', 'ied_', 'anti', 'Mad', 'lly_', 'Is_', '7_', 'ess', 'bunch_', 'vin', 'slow_', 'style', 'hi', 'eyes_', 'cinema_', 'showing_', 'gen', 'ra_', 'among_', 'unc', 'Po', 'Peter_', 'kid_', 'ght_', 'ny_', 'gh', 'tro', 'four_', 'ue', 'ley_', 'stuff_', 'strange_', 'sit_', 'sch', 'anyway', '199', 'hours_', 'These_', 'Most_', 'own', 'ned_', 'ban', 'Fa', 'decided_', 'xi', 'top', 'll', 'get', 'events_', 'Also_', 'typical_', 'shots_', 'look', 'happy_', 'um_', 'simple_', 'either', 'comment', 'ssi', 'ps', 'Bar', 'Per', 'saying_', 'none_', 'surprised_', 'sse', 'ka', 'ily_', 'horror', 'dig', 'tt', 'ric', 'post', 'TV', '198', '* ', 'half', 'gn', 'ste', 'ls', 'hero_', 'Pi', 'Like_', 'sad_', 'hear_', 'begins_', 'rent_', 'ure', 'rie', 'greatest_', 'Je', 'van', 'sci', 'kid', 'himself', 'Also', 'view_', 'score_', 'dge', 'became_', 'Cra', '197', 'ones_', 'cal', '9_', 'hor', 'hand_', 'days_', 'yourself_', 'tle', 'gan', 'ea', 'ago', 'WA', 'pen', 'ls_', 'learn_', 'Sta', 'By_', 'middle_', 'job', 'uc', 'ko', 'bar', 'lots_', 'cheap_', 'fi', 'stay_', 'stand_', 'pri', 'za', 'im', 'ight', 'happen_', 'Ab', 'Gar', 'ore', 'lan', 'classic', 'writer_', 'ster', 'picture', 'hate_', 'der', 'grand', 'disc', 'Mi', 'ud', 'é', 'murder_', 'basically_', 'jokes_', 'famous_', 'eg', 'easy_', 'rm', 'der_', 'R_', 'Mat', 'two', 'daughter', 'Spi', 'camera', 'AN', 'glo', 'talk_', 'daughter_', 'Fre', 'ri', 'perfect', 'experience_', 'buy_', 'zo', 'bu', 'Pu', 'Col', 'uni', 'later', 'children', 'sets_', 'annoying_', 'Tom_', 'uses_', 'jo', 'dead', 'psycho', 'mid', 'room', 'ki', 'hope', 'dialogue', 'attention_', 'cc', 'above_', 'possibly_', 'mo', 'difficult_', 'Mon', 'Japanese_', '!\" ', 'death', 'class_', ': \"', 'tic', 'ler', 'bus', 'genre_', 'stre', 'keeps_', 'cre', 'una', 'tly_', 'leaves_', 'RE', 'yes', 'realize_', 'nor_', 'figure_', 'Chan', 'rec', 'minute_', 'leading_', 'high', 'gui', 'ug', 'sequence_', 'na_', 'help', 'ani', 'Who_', 'exist', 'documentary_', 'sal', 'pe', 'key_', 'Bra', 'murder', 'leg', 'songs_', 'production', 'dle', 'cla', 'arm', 'US', \"'. \", 'reason', 'moving_', 'alone_', 'Ko', 'Bel', 'fu', 'elements_', 'Ste', 'prof', 'ning_', 'ey', 'dark', 'tur', 'les_', 'Ni', 'NOT_', 'ps_', 'bor', 'ary_', ' />\"', 'tter', 'level_', 'ys', 'apparently_', 'poorly_', 'meets_', 'killing_', 'id', 'ging_', 'ep', 'emotional_', 'brings_', 'means_', 'fla', 'episodes_', 'doubt_', 'camp', 'ME', 'Ad', 'sen', 'opinion', 'nch', 'ell', 'Ri', 'writer', 'something', 'Fe', 'flick_', 'flaw', 'ath', 'net', 'lines', 'cinematography_', 'straight_', 'slow', 'lu', 'ber', 'shi', 'husband_', 'forward_', 'form_', 'cra', 'ay', 'Fo', 'Another_', 'wo', 'whom_', 'reality_', 'hold_', 'Chi', 'Bro', 'roles_', 'move_', 'fire', 'brother_', 'Gi', 'Ben', 'review', 'que', 'cri', 'television_', 'overall_', 'French_', 'violence_', 'lla', 'enti', 'ass', 'previous_', 'forced_', 'cop', 'Oscar_', 'DE', 'possible_', 'hat', 'ear', 'budget', 'Tu', 'Ber', 'start', 'nti', 'hard', 'yn', 'school', 'deal_', 'rest', 'problems_', 'lie', 'ite', 'cool_', 'add_', 'towards_', 'reading_', 'LO', 'Gold', 'regard', 'itself', 'OK', 'leads_', 'id_', 'ved_', 'moments', 'dia', 'aw', '!) ', ' $', 'write_', 'theme_', 'Wo', 'filmed_', 'use', 'talent_', 'silly_', 'personal_', 'performances', 'needed_', 'mit', 'meant_', 'cli', 'Sho', 'tain', 'Pri', 'whi', 'comments_', 'city_', 'various_', 'sing', 'rate_', 'create_', 'respect', 'port', 'act', '194', 'message_', 'ted', 'dance_', 'case', 'ves_', 'song_', 'somehow_', 'incredibly_', 'points_', 'manages_', 'career_', 'begin_', 'Tra', 'RI', '20_', 'lai', 'interested_', 'terrible', 'hell_', 'har', 'Ku', 'Ger', 'video', 'ren', 'ky_', 'Ap', 'review_', 'ds', 'blood', 'worse', 'new', 'des', 'ways_', 'read', 'herself_', 'fre', ' *', 'set', 'rated_', 'friends', 'feature_', 'eventually_', 'blood_', 'Sea', 'ving_', 'enjoyable_', 'appear_', 'Stan', 'SE', 'thought', 'suit', 'qui', 'political_', 'person', 'les', 'gla', 'around', 'think', 'len', 'hit', 'direction', 'tale_', 'mess', 'dramatic_', 'ual_', 'gore_', 'Can', 'Am', 'ver_', 'others', 'ju', 'fairly_', 'dan', 'power_', 'dro', 'count', 'Her', 'une', 'third_', 'rop', 'crap', 'ai', 'ade', 'Joe_', 'town', 'ridiculous_', 'gone_', 'William_', 'particular_', 'older_', 'male_', 'humor', 'ard_', 'where', 'run', 'ld', 'bb', 'C_', 'ther_', 'sp', 'plenty_', 'ling', 'future_', 'stars', 'sin', 'pi', 'meet_', 'lt', 'da_', 'check_', 'En', '?\" ', 'ball', 'animation_', 'ta_', 'King_', 'hardly_', 'cul', '60', 'rt', 'Is', 'rai', 'land_', 'clu', 'wise', 'fast_', 'class', 'bra', 'worked_', 'question', 'per_', 'ok', 'expecting_', 'front_', 'come', 'Cu', 'scary_', 'past', 'hero', 'Mel', 'gri', 'average_', 'writers_', 'nk', 'fashion', 'dream', 'bear', 'attempts_', 'stand', 'total_', 'through', 'sm', 'ms', 'ice', 'gs_', 'eye', 'effort_', 'ale', 'warm', 'note', 'ger_', 'follow_', 'cro', 'vis', 'subject_', 'reviews_', 'mm', 'ect', 'Wa', 'Rob', 'imagine_', 'however_', 'decides_', 'brother', 'achieve', 'things', 'stage_', 'sound', 'rating_', 'ously_', 'ier', 'features_', 'ase', 'Vo', 'really', 'pay', 'pal', 'filled_', 'Disney_', 'telling_', 'join', 'coa', 'Lee_', 'team_', 'ov', 'emp', 'days', 'bin', 'ann', 'ally', 'women', 'social_', 'friend', 'vic', 'novel_', 'gle', 'ance_', 'weak_', 'viewers_', 'sy', 'fort', 'idea', 'Mu', 'MA', 'thriller', 'medi', 'forget_', 'York_', 'Au', 'stuff', 'ons', 'hilarious_', 'career', 'Ke', 'Christ', 'ors_', 'mentioned_', 'mark', 'def', 'watching', 'version', 'lor', 'flo', 'country_', 'G_', 'Bat', 'plain_', 'Sam', 'Anyway', 'lic', 'expected_', 'Tru', 'Great_', 'Ser', 'N_', 'And', '?) ', 'san', 'hr', 'Ham', 'pay_', 'lea', 'hol', 'Unfortunately', 'Luc', 'uti', 'row', 'history', 'bea', 'What', 'Or_', 'unless_', 'ica', 'episode', 'stra', 'sounds_', 'ability_', 'Cha', 'sco', 'represent', 'portrayed_', 'outs', 'dri', 'crap_', 'Oh', 'word_', 'open_', 'fantastic_', 'II', 'power', 'ical_', 'badly_', 'Well_', 'IN', 'Angel', 'waiting_', 'sees_', 'mor', 'ari', 'tom', 'sli', 'nation', 'mi', 'inf', 'Mil', 'viewing_', 'rt_', 'premise_', 'ma_', 'fit_', 'wl', 'unique_', 'talent', 'stay', 'fails_', 'breath', 'thi', 'ert', 'Sco', 'talk', 'slightly_', 'je', 'ah', 'NE', 'Fin', 'ridiculous', 'la_', 'Ki', 'vir', 'hea', 'ely_', 'beautiful', 'admit_', 'pu', 'crime_', 'comment_', '0_', 'shot', 'free_', 'entertaining', 'deserves_', 'mas', 'dialog_', 'hip', 'ff_', 'talented_', 'runs_', 'ini', 'ew', 'ded', 'Gri', 'roles', 'realistic_', 'clo', 'ana', 'Rat', 'Oh_', 'Man_', 'Den', 'spent_', 'rse', 'die_', 'Spe', 'Dra', 'ord', 'mal', 'ism', 'del', 'War', 'Cro', 'nn', 'min', 'fighting_', 'excellent', 'ct_', 'ask_', 'abo', 'parents_', 'ou', 'flash', 'Ver', 'Star', 'ym', 'score', 'nature_', 'den_', 'cou', 'body', 'aff', 'Ze', 'Pat', 'Mal', 'lab', 'wing_', 'theater_', 'sho', 'ow', 'mini', 'biggest_', 'Best_', 'wrote_', 'perfectly_', 'pack', 'ile', 'bly_', 'agree_', 'Perhaps_', '-- ', 'sign', 'di', 'cer', 'caught_', 'Good_', 'visual_', 'roll', 'my', 'memorable_', 'kids', 'ise_', 'hin', 'bre', 'beat', 'ring', 'reveal', 'res', 'pit', 'fa', '70', 'words_', 'wn', 'wait_', 'storyline_', 'make', 'ended_', 'ship_', 'ose', 'hot_', 'add', 'DO', 'ib', 'eri', 'directors_', 'amount_', 'Sure', 'ua', 'tin', 'mu', 'hilarious', 'eti', 'deep_', 'battle_', 'bas', 'Pre', 'Ali', 'tre', 'tie', 'thriller_', 'spirit', 'sister', 'ship', 'ser', 'rl', 'rich_', 'outside_', 'ato', 'ad_', 'Do', 'weren', 'sla', 'ro_', 'large_', 'craft', 'Shi', 'ye', 'true', 'spend_', 'rd', 'entirely_', 'Do_', 'wit', 'quickly_', 'powerful_', 'ary', 'Jane_', '193', 'sti', 'ph', 'mel', 'list', 'interest', 'footage_', 'comm', 'Tri', 'vers', 'spe', 'sna', 'sequences_', 'present', 'casting_', 'Star_', 'M_', ').', 'shoot', 'result_', 'gre', 'fore', 'ete', 'break', 'soundtrack_', 'sion_', 'poor', 'lay', 'eas', 'black', 'temp', 'nda', 'king', 'compared_', 'chu', 'break_', 'Ben_', 'ute', 'recent_', 'pure_', 'oi', 'lie_', 'burn', 'uns', 'rip', 'ner_', 'late', 'husband', 'former_', 'dull_', 'argu', 'Hollywood', 'nc', 'ming_', 'lin', 'atmosphere_', 'wood', 'why', 'amazing', 'ron', 'rat', 'gra', 'sed_', 'period_', 'game', 'Sto', 'win_', 'ult', 'scar', 'pun', 'hei', ' `', 'release_', 'present_', 'pin', 'ks_', 'appreciate_', '00', 'jump', 'bomb', 'HA', 'showed_', 'nan', 'kills_', 'decade', 'NO', 'Boy', 'ting', 'rating', 'editing_', 'actress', 'Wal', 'Ea', '\", \"', 'weird_', 'inside_', 'hair', 'eli', 'disappointed_', 'Wor', 'ski', 'ings_', 'fast', 'drag', 'adapt', 'TO', 'NG_', 'sequel_', 'fle', 'Sand', 'RO', 'whatever_', 'sleep', 'sca', 'ret', 'ney_', 'creepy_', 'cal_', '\") ', 'sor', 'popular_', 'nne', 'kick', 'ht', 'display', 'another', 'ves', 'please_', 'moves_', 'care', 'bet', 'bat', 'War_', 'CO', 'program', 'predictable_', 'positive_', 'hing_', 'copy_', 'bia', 'anything', 'affect', 'thrill', 'rk', 'mark_', 'ism_', 'edit', 'Bri', 'rate', 'missing_', 'ila', 'ial', 'guess', 'ft', 'entr', 'decide_', '30', 'sun', 'filmmakers_', 'box_', 'ating_', 'Cla', 'CA', '18', 'nie', 'material_', 'married_', 'hu', 'fin', 'blo', 'Wood', 'Tom', 'vi', 'oni', 'ena', 'BA', 'path', 'os_', 'human', 'mag', 'ins', 'earlier_', 'TI', 'LA', 'Far', 'portrayal_', 'orc', 'lame_', 'ks', 'form', 'call', 'acted_', 'Christmas_', 'violence', 'superb_', 'idiot', 'follow', 'blow', 'SO', 'Les', 'Bill_', '30_', 'sorry_', 'created_', 'common_', 'cheesy_', 'Lea', 'Carl', '!!! ', 'question_', 'pt', 'pick', 'med_', 'leaving_', 'box', 'Ci', 'Bla', 'AR', '\".<', 'ze_', 'makers_', 'draw', 'ala', 'Day', 'B_', 'succeed', 'pat', 'ones', 'gay_', 'cy', 'barely_', 'ara', 'air_', 'San', 'Director_', 'xt', 'screenplay_', 'pan', 'miss_', 'does', 'consider_', 'com', 'ER', 'ub', 'ple', 'mystery_', 'mine', 'involving_', 'familiar_', 'Mari', 'German_', 'nat', 'eye_', 'dly_', 'disa', 'country', 'att', 'app', 'tho', 'press', 'mat', 'llo', 'fi_', 'connect', 'called', 'ane', 'May', 'LE', 'K_', 'Italian_', 'Every_', 'sure', 'ster_', 'starring_', 'horse', 'further_', 'entertainment_', 'ense', 'dog', 'disappointed', 'cher', 'af', 'won_', 'secret', 'likes_', 'indi', 'follows_', 'ball_', 'God_', 'Cur', '196', 'wasted_', 'ideas_', 'cur', 'Bal', 'lly', 'ire', 'gu', 'general_', 'believable_', 'aus', 'Stu', 'Despite_', 'understand', 'lit', 'last', 'cy_', 'bought_', 'ago_', 'Very_', 'Only_', 'Han', 'wear', 'thu', 'themselves', 'recently_', 'ms_', 'intention', 'focus_', 'ations_', 'ali', 'yp', 'yet', 'ici', 'gy', 'exten', 'Min', 'Lin', 'Ed', 'Dar', 'tis', 'credits_', 'Now', '50', 'sister_', 'setting_', 'odd_', 'missed_', 'mea', 'lot', 'ight_', 'gg', 'fantasy_', 'ash', 'US_', 'Overall', 'young', 'suddenly_', 'nge', 'members_', 'dra', 'cover_', 'artist', 'Watch_', 'moment', 'background_', '.....', 'seriously_', 'mic', 'considered_', 'Ric', 'Pres', '! <', ' (\"', 'opinion_', 'ise', 'gun', 'different', 'Sou', 'utterly_', 'asse', 'alt', 'Though_', 'LY_', 'Big_', 'situation_', 'rio', 'il_', 'ef', 'ding', 'Still', 'Cre', 'younger_', 'special', 'raise', 'El', '90', 'walk_', 'tone_', 'tes_', 'sitting_', 'glad_', 'base', 'Let', 'Boo', 'vent', 'lead', 'considering_', 'animated_', 'witness', 'torture', 'throw', 'sea', 'load', 'lim', 'hot', 'following_', 'ess_', 'center', 'Scott_', 'NG', 'BO', '15_', 'word', 'rid', 'pop', 'ions', 'ges', 'enter', 'Sal', 'Gre', 'ties_', 'spl', 'hy', 'ery_', 'disappointment', 'avoid_', 'Jud', 'Ce', 'need', 'hel', 'hands_', 'develop', 'cause_', 'Steve_', 'zombie_', 'voice', 'successful_', 'eo', 'Mary_', 'EN', 'Because_', 'stage', 'rv', 'master', 'crazy_', 'Mer', 'rent', 'hes', 'OF_', 'yl', 'tive_', 'remake_', 'passion', 'managed_', 'fra', 'fans', 'drive', 'CH', 'Blo', 'Art', 'surprise_', 'suggest', 'list_', 'imme', 'crew_', 'continu', 'Sci', 'solid_', 'ora', 'eu', 'Men', 'Cal', 'sus', 'shar', 'omi', 'ita', 'istic_', 'Pl', 'Jack', 'Davi', 'wonder', 'slasher_', 'produced_', 'frame', 'cle', 'Em', 'subs', 'state', 'seek', 'ona', 'mention', 'laughing_', 'iti', 'hide', 'date', 'Some', 'touch', 'soft', 'shop', 'interview', 'dumb_', 'clean', 'bored_', 'bill', 'bed_', 'beauty_', 'basic_', 'Cou', 'zi', 'ultimately_', 'thinks_', 'sto', 'odd', 'masterpiece', 'kind', 'cool', 'Ac', 'tto', 'sit', 'nci', 'ized_', 'gore', 'dee', 'boo', 'Va', 'Come', 'ning', 'escape', 'eng', 'RA', 'America', 'worthy_', 'unre', 'tche', 'shame_', 'nothing', 'explo', 'Sl', 'Bus', 'BE', '13', 'pra', 'least', 'effect_', 'deliver', 'boys_', 'Wi', 'Stra', 'Fr', 'Cap', '**', '\".', 'space_', 'potential_', 'oli', 'lon', 'ind', 'gor', 'gon', 'generally_', 'ext', 'chees', 'beginning', 'Tony_', 'wait', 'meaning', 'ley', 'fire_', 'des_', 'cop_', 'ati', 'Ram', 'Ex', '195', 'were', 'survive', 'ral_', 'push', 'mut', 'killer', 'dist', 'charm', 'ang', 'Frank', 'writing', 'worth', 'wor', 'stop', 'stick_', 'ler_', 'chemistry_', 'cap', 'ae', 'Ya', 'second', 'ost', 'machine', 'lessly_', 'individual', 'experience', 'ead', 'dancing_', 'Sy', 'Del', 'Bor', '!!', 'would', 'suspense_', 'project', 'intelligent_', 'cover', 'asi', 'Brit', 'speak_', 'season_', 'oth', 'ida', 'factor', 'amo', 'World_', 'Once_', 'Hard', ' ... ', 'tol', 'live', 'changed_', 'brain', 'uri', 'seriously', 'release', 'likely_', 'gne', 'explain_', 'ance', 'added_', 'Here_', 'AL', '% ', 'wre', 'spar', 'gree', 'eyes', 'detail', 'Night', 'Mag', 'term', 'tape', 'public_', 'pleas', 'lives', 'ker', 'ile_', 'had', 'dre', 'directing_', 'dialog', 'convincing_', 'chance', 'big', 'beat_', 'appl', 'truth_', 'spa', 'rica', 'monster_', 'market', 'imm', 'have', 'fine', 'clue', 'card', 'blu', 'adult_', 'Who', 'Jim_', 'Bea', '.)', 'value', 'twist_', 'thrown_', 'phe', 'model', 'entertainment', 'Where_', 'LI', 'Ju', 'Black_', 'ura', 'nic', 'han', 'failed_', 'cinematic_', 'bizarre_', 'ben', 'Gu', 'rare_', 'mbo', 'historical_', 'everyone', 'epi', 'ate', 'ada', 'Cli', 'wind', 'sou', 'nder', 'mb', 'held_', 'formula', 'flu', 'effect', 'clever_', 'catch_', 'W_', 'pick_', 'business_', 'attempt', 'Show', 'Paul', 'segment', 'romance_', 'ram', 'nom', 'how', 'ged_', 'flow', 'equally_', 'computer_', 'commercial', 'Val', 'IMDb_', 'trans', 'sent_', 'pet', 'lk', 'ider', 'corn', 'channel', 'Ge', 'Christopher_', 'ways', 'tat', 'subject', 'shooting_', 'return_', 'neither_', 'neighbor', 'lady_', 'impossible_', 'Spa', 'BI', '***', ' -', 'yr', 'violent_', 'syn', 'suffer', 'fur', 'cru', 'Charl', 'secret_', 'rp', 'ros', 'pie', 'ious_', 'hoping_', 'ence_', 'Ye', 'Son', 'trick', 'nia', 'effective_', 'desp', 'costume', 'check', 'board_', 'ami', 'aire', 'ado', 'Whi', 'Two_', 'Rose', 'Green', 'surround', 'promise', 'mad', 'lesson', 'imagination', 'hum', 'excuse_', 'escape_', 'aspect_', 'ak', 'Thu', 'Pal', 'Kr', 'Bur', 'vil', 'travel', 'reso', 'protagonist', 'object', 'nes', 'longer_', 'lia', 'key', 'incredible_', 'hoo', 'fool', 'expression', 'bot', 'bel', 'Ree', 'Oscar', 'Fu', 'safe', 'remains_', 'note_', 'natural_', 'just', 'hm', 'grace', 'credit_', 'constantly_', 'Sam_', 'Ren', 'OK_', 'view', 'unlike_', 'surprise', 'success_', 'ssion', 'song', 'player', 'match_', 'ela', 'din', 'critic', 'accident', '20', 'otherwise_', 'material', 'knowing_', 'ings', 'ffe', 'depth_', 'cula', 'Whe', 'Ph', 'Ai', 'respect_', 'puts_', 'pher', 'kin', 'concept_', 'zed_', 'unfortunate', 'que_', 'predictable', 'order', 'onto_', 'meta', 'ev', 'dress', 'dog_', 'cell', 'Thi', 'Frank_', 'spin', 'rot', 'military_', 'hall', 'cut', 'choice_', 'chick', 'bs', 'Za', 'Many_', 'witch', 'weak', 'swa', 'rti', 'producers_', 'inn', 'gold', 'fault', 'ez', 'cute_', 'cult_', 'WO', 'SH', 'drink', ', (', 'wall', 'theme', 'taste', 'sion', 'iz', 'gun_', 'ek', 'drawn_', 'anyone', 'antic', 'tension_', 'team', 'sweet_', 'ree', 'perform', 'partner', 'horrible', 'contains_', 'Es', 'De_', 'Chris_', 'AT', 'vote', 'tch_', 'singing_', 'shine', 'hasn', 'happen', 'gal', 'demon', 'dar', 'Jer', 'GE', 'ske', 'indeed_', 'guys', 'emotion', 'apart_', 'See', 'Roger', 'Pol', 'trouble_', 'seat', 'planet', 'exciting_', 'err', 'dream_', 'cus', 'arrive', 'HO', '!!!!', 'trip_', 'today_', 'sle', 'setting', 'rr', 'plus_', 'og', 'faci', 'disp', 'crack', 'cen', 'Gun', 'words', 'will', 'prefer', 'pect', 'noi', 'leader', 'dit', 'deal', 'creep', 'Zo', 'Sid', 'East', 'record', 'poo', 'normal_', 'message', 'ffi', 'fer', 'correct', 'colle', 'ator', 'Ros', 'Other_', 'zen', 'usi', 'pil', 'mental_', 'ji', 'immediately_', 'ible_', 'capt', 'bab', 'Chu', 'tar', 'stands_', 'progress', 'making', 'lc', 'fic', 'exp', 'encounter', 'circ', 'change', 'annoying', 'Mur', 'Lor', 'Little_', 'tl', 'rain', 'fail', 'died_', 'Time', 'Blood', 'tell', 'reflect', 'ked_', 'judge', 'ide', 'development_', 'control_', 'clima', 'bed', 'alr', 'Tre', 'trouble', 'thr', 'spot', 'ress', 'red', 'pol', 'hill', 'eb', 'TH', 'Ken', '\\x85 ', 'surprisingly_', 'rep', 'freak', 'dep', 'college_', 'brilliant', 'blin', 'bath', 'People_', 'Nat', 'Charles_', 'walking_', 'ref', 'reco', 'pace_', 'nde', 'mil', 'mainly_', 'literally_', 'fia', 'dull', 'Sn', 'Ever', 'Dam', 'Bre', 'Brad', 'Both_', 'ward', 'trash', 'tough_', 'serve', 'reasons_', 'ngs', 'llen', 'ines', 'honest', 'focus', 'carrie', 'aim', 'Us', 'Prince', 'Nothing_', 'truth', 'supp', 'sma', 'musical', 'inco', 'fight', 'enc', 'bother', 'arch', 'Jon', 'Japan', 'Er', 'Des', '!!!', 'unw', 'unfortunately_', 'til', 'rese', 'marri', 'ior', 'ene', 'ain_', 'Aust', 'ular', 'tru', 'tch', 'tale', 'prop', 'phan', 'orat', 'nit', 'matter', 'host', 'hood', '\\\\&undsc', 'Not', 'Film_', 'Ama', 'yle', 'var', 'standards', 'pers', 'nice', 'meaning_', 'laughs_', 'joke_', 'iss', 'happi', 'era_', 'WH', 'Lil', 'Girl', 'ES', ' />-', 'watche', 'tant', 'qua', 'presented_', 'minor_', 'gro', 'fie', 'door', 'corp', 'catch', 'cally_', 'bert', 'Indian_', 'Gen', 'questions_', 'lacks_', 'forever', 'establish', 'esc', 'cheap', 'Sol', 'while', 'twist', 'society_', 'pass_', 'overa', 'merely_', 'highlight', 'flat_', 'fill', 'color', 'cartoon_', 'Will_', 'NT', 'IT', 'Harry_', 'Fan', 'youth', 'possible', 'orm', 'free', 'eight', 'destroy', 'creati', 'cing_', 'ces_', 'Carr', 'unl', 'suggest_', 'slo', 'owner', 'kh', 'instead', 'influence', 'experiment', 'convey', 'appeal_', 'Ol', 'Night_', '---', 'vy', 'terms_', 'sick_', 'par', 'once', 'law', 'ize_', 'infe', 'Spo', 'House_', '\\x85', 'studio_', 'simple', 'rre', 'guard', 'girlfriend_', 'fear', 'dam', 'concern', 'amusing_', 'adaptation_', 'Ms', 'King', 'water', 'ory_', 'officer', 'litera', 'knock', 'grat', 'falling_', 'ered_', 'cow', 'cond', 'alo', 'Kar', 'Der', 'Cri', 'text', 'skin', 'sequel', 'level', 'impression_', 'ice_', 'force_', 'fake_', 'deri', 'contain', 'band_', 'appa', 'South_', 'HE', 'Conn', 'wise_', 'ur_', 'ual', 'sy_', 'luck', 'lack', 'impressi', 'disaster', 'business', 'being', 'beg', 'Burt', ' <', 'villain_', 'type', 'shoot_', 'shame', 'sb', 'pt_', 'proves_', 'manner', 'lame', 'impressive_', 'ern', 'disappear', 'alone', 'LL', 'Having_', 'Brook', 'Arm', '!\"', 'works', 'state_', 'shock', 'rev', 'mus', 'int', 'ino', 'images_', 'brid', 'berg', 'alis', 'Clo', 'singer', 'shr', 'rock_', 'provides_', 'page', 'instance', 'drug_', 'crime', 'beautifully_', 'acts_', 'UN', 'Tal', 'Bruce_', 'self_', 'reality', 'mans', 'lived_', 'innocent_', 'ically_', 'fall', 'dict', 'Henry_', 'Fox', 'Bac', 'sold', 'says', 'period', 'ome', 'melodrama', 'include_', 'evil', 'Ins', 'stati', 'silent_', 'ria', 'mom', 'met_', 'guns', 'ground', 'gate', 'fell_', 'cle_', 'cari', 'birth', 'Look', 'Hill', '1950', 'water_', 'reminded_', 'express', 'delight', 'als_', 'Wes', 'Mis', 'Louis', 'Grant', 'xe', 'written', 'touch_', 'ters_', 'squa', 'moral', 'ffer', 'aut', 'appearance_', 'Sim', 'Nor', 'Mont', 'IS_', 'Cath', 'take', 'shel', 'protect', 'gut', 'ans', 'Too_', 'Scar', 'Death', 'American', 'AND_', 'throw_', 'suck', 'standard_', 'sil', 'should', 'share_', 'scary', 'loves_', 'indu', 'foot', 'ew_', 'answer', 'Wit', 'Van_', 'Terr', 'Str', 'subtle_', 'stories', 'store_', 'must', 'ments_', 'mbi', 'gs', 'ft_', 'fellow_', 'erat', 'eni', 'crash', 'ches', 'becoming_', 'appeared_', 'TE', 'Fal', '., ', 'visit', 'viewer', 'tag', 'surely_', 'sur', 'stri', 'putting_', 'pull_', 'process', 'pointless_', 'nta', 'mass', 'hur', 'hell', 'gue', 'girls', 'Rev', 'Pan', 'Billy_', 'villain', 'suppose_', 'sick', 'prom', 'narrat', 'mer_', 'followed_', 'decision', 'auto', 'adult', 'Movie_', 'Ban', 'tone', 'thoroughly_', 'sympath', 'sts_', 'sk', 'pot', 'piece', 'offers_', 'nte', 'most', 'helps_', 'det', 'cti', 'brief_', 'block', 'adds_', 'Street', 'Red_', 'Qui', 'Love', 'BL', 'support_', 'ses_', 'rta', 'recognize', 'mission', 'ignore', 'hon', 'broad', 'bid', 'ano', 'Swe', 'Shakespeare', 'Ron', 'Mart', 'Charlie_', 'thanks_', 'tage_', 'serial_', 'revenge_', 'ors', 'office_', 'nst', 'feature', 'drugs', 'disturb', 'anymore', 'Bl', \", '\", 'univers', 'touching_', 'strange', 'improve', 'iff', 'heavy_', 'fare', 'central_', 'buff', 'Inter', 'EA', 'worr', 'turning_', 'tired_', 'than', 'seemingly_', 'motion_', 'ku', 'has', 'goe', 'evi', 'duc', 'dem', 'cinematography', 'aspects_', 'any', 'High', 'Cho', 'tick', 'surviv', 'suicide', 'return', 'remember', 'ppy_', 'noti', 'mess_', 'mes', 'inve', 'grow', 'enge', 'dom', 'Tar', 'Since_', 'Roy', '19', ' ( ', 'track_', 'racis', 'narrative_', 'nal', 'mysterious_', 'moral_', 'imp', 'desert', 'compl', 'along', 'Sw', 'Super', 'HI', 'Dor', 'America_', 'vert', 'superb', 'stu', 'shouldn', 'science_', 'rough', 'ray', 'ova', 'dumb', 'deb', 'court', 'control', 'complex_', 'butt', 'Joe', 'Ir', 'Direct', 'throughout', 'tende', 'stic_', 'somewhere_', 'sel', 'pti', 'picked_', 'parts', 'mob', 'fear_', 'developed_', 'couple', 'cas', 'attitude', 'apo', 'Sun', 'MO', 'L_', 'Ei', 'teen_', 'pull', 'ough', 'hunt', 'favor', 'dos', 'delivers_', 'chill', 'ately', 'Van', 'vat', 'tz', 'trip', 'stuck_', 'rela', 'mood_', 'finish', 'essen', 'ering_', 'disappoint', 'could', 'commit', 'TA', 'Lam', 'Harris', 'whole', 'value_', 'ural', 'sim', 'season', 'redeeming_', 'poli', 'please', 'happened', 'geo', 'force', 'ero', 'core_', 'cand', 'blue', 'bell', 'assi', 'asp', 'adventure_', 'Sin', 'McC', 'whatsoever', 'sky', 'shows', 'pse', 'language_', 'insight', 'ier_', 'finding_', 'everything', 'cker', 'challenge', 'books_', 'Out', 'Ji', 'Glo', 'tune', 'terri', 'prem', 'oe', 'nish', 'movement', 'ities_', 'effort', 'absolute_', 'Brian_', 'Alan_', 'unin', 'unde', 'ude', 'tear', 'oh_', 'ize', 'ilia', 'hint', 'credib', 'craz', 'choice', 'charming_', 'audiences_', 'apart', 'York', 'Marc', 'wonderful', 'willing_', 'wild', 'repeated', 'refer', 'ready_', 'radi', 'punch', 'prison', 'painful_', 'pain', 'paid_', 'pace', 'nni', 'mate_', 'hole', 'future', 'disturbing_', 'cia', 'buck', 'ache', 'Taylor', 'Lind', 'Hol', 'vel', 'tor_', 'terrific_', 'suspense', 'sf', 'research', 'remark', 'problem', 'plu', 'pathetic_', 'negative_', 'lovely_', 'lift', 'hype', 'gl', 'earn', 'ave', 'Their_', 'SS', 'Cass', 'slowly_', 'rented_', 'opportunity_', 'fat', 'every', 'este', 'dub', 'cons', 'bull', 'Sav', 'P_', 'My', 'wondering_', 'unbe', 'twe', 'statu', 'shin', 'rock', 'party_', 'inform', 'heroine', 'hate', 'girlfriend', 'fate', 'ette', 'dies_', 'comparison', 'alb', 'ak_', 'Lis', 'Christian_', 'Act', 'yon', 'storyline', 'soul', 'rece', 'rea', 'product', 'nut', 'lets_', 'funniest_', 'field_', 'city', 'Stephen_', 'GH', 'Ann', 'wee', 'weapon', 'viewing', 'tte', 'sty', 'spi', 'quality', 'price', 'possess', 'ntly', 'dd', 'compa', 'buy', 'agree', 'Hal', 'Comp', 'twists_', 'shak', 'nudity_', 'mati', 'giant_', 'company_', 'baby_', 'admit', 'Finally', 'wn_', 'whe', 'romance', 'presence_', 'myself', 'jokes', 'ident', 'friendship', 'fift', 'explore', 'episodes', 'element_', 'edi', 'eat', 'conve', 'Ira', 'However_', 'DI', 'winning_', 'sexy_', 'rescue', 'physical_', 'pe_', 'oid', 'nobody_', 'nis', 'mad_', 'lin_', 'ket', 'hom', 'generation', 'dance', 'attack', 'appropriate', 'allowed_', 'Ve', 'RS', 'Mr_', 'Kid', 'Instead_', 'Hell', 'Everything_', 'Before_', 'Arthur_', 'waste', 'themes_', 'stunt', 'rap', 'million_', 'hi_', 'games', 'fair_', 'distract', 'cross', 'boat', 'available_', 'abilit', 'Hitler', 'Fl', 'Cas', 'wearing_', 'spirit_', 'rede', 'rb', 'perspective', 'ocr', 'mac', 'kle', 'gang_', 'floor', 'fab', 'Pen', 'ON', 'Kur', 'Jerry_', 'Here', 'Andrew', '??', 'window', 'uss', 'mp_', 'intens', 'expert', 'ei', 'changes_', 'carry_', 'born_', 'bee', 'award', 'Sor', 'Jos', 'Home', 'Cat', '1980', 'zing_', 'victim', 'tight', 'space', 'slu', 'pli', 'neat', 'mistake', 'ky', 'joke', 'includes_', 'hear', 'emb', 'dev', 'damn_', 'confusi', 'church', 'NI', 'Clark', 'theatre', 'sso', 'lock', 'laughed_', 'fran', 'drive_', 'danger', 'alle', 'Which_', 'Western', 'Roman', 'Rit', 'Pie', 'Law', 'France', 'Did_', '14', 'vor', 'usual', 'turn', 'supposedly_', 'sm_', 'satisf', 'realistic', 'pieces_', 'nse', 'near', 'image_', 'flat', 'development', 'design', 'contrast', 'colla', 'board', 'arti', 'anywhere', 'Unfortunately_', 'Rock', 'Ford', 'Doc', 'white', 'small', 'replace', 'prison_', 'owe', 'minat', 'may', 'inspired_', 'helped_', 'expect', 'doll', 'dish', 'chase', 'awa', 'Those_', 'Second', 'OR', 'Nazi', 'Ell', 'watchable', 'via', 'test', 'stick', 'step_', 'speech', 'relationship', 'pass', 'ote', 'nel', 'mild', 'gue_', 'embarrass', 'describe_', 'bound', 'bother_', 'aging', 'Julie', '70s', 'via_', 'street_', 'squ', 'scream', 'pos', 'overs', 'mix_', 'martial_', 'magic_', 'jud', 'gener', 'eh', 'concept', 'alien', 'FO', 'which', 'values_', 'success', 'soldiers_', 'pla', 'lous', 'lose_', 'io', 'ike', 'fish', 'eth', 'ddy', 'crowd', 'creative_', 'conc', 'beh', 'bbi', 'Matth', 'Europe', '1970', 'ulat', 'track', 'target', 'swea', 'stal', 'refuse', 'phon', 'pho', 'hang', 'gea', 'doubt', 'compr', 'cloth', 'cliché', 'bland', 'behavior', 'aci', 'Simp', 'Leon', 'England', 'Edi', 'Cons', ')<', ' .', 'wy', 'worker', 'volu', 'vehicle', 'tour', 'random_', 'phone_', 'ong', 'moved_', 'grave', 'folk', 'filming_', 'feelings_', 'build_', 'basi', 'Tor', 'TR', 'Sk', 'New', 'Miss_', 'Kl', 'Kat', 'Boll', 'zil', 'ust', 'robot', 'result', 'reac', 'ped', 'pea', 'ow_', 'mmi', 'laughs', 'issues_', 'intended_', 'impressed_', 'favorite', 'dw', 'documentary', 'doctor_', 'debut', 'account', 'North', 'Im', 'GO', 'weird', 'transform', 'train', 'swi', 'sum', 'soci', 'same', 'reh', 'ld_', 'ffic', 'conversation', 'comedic_', 'artistic_', 'adi', 'accept', 'Stone', 'Jew', 'CR', 'threaten', 'stea', 'scra', 'sake', 'potential', 'listen', 'het', 'cted_', 'cod', 'chase_', 'berg_', 'appear', 'Ton', 'Queen', 'Mark_', 'Hall', 'FI', 'wer', 'thes', 'sons', 'provide_', 'nger', 'ney', 'mot', 'mask', 'flesh', 'exe', 'dozen', 'disgu', 'conclusion', 'accent', 'Victoria', 'SP', 'Jr', 'Char', 'Albert', 'try', 'tal_', 'round_', 'mix', 'ison', 'hundred', 'holds_', 'gger', 'approach_', 'Space', 'Okay', 'MI', 'Love_', 'Elvi', 'Doo', 'tragic_', 'sweet', 'stud', 'sible', 'remain', 'pur', 'nts_', 'ken', 'got', 'fam', 'edge_', 'Hea', 'Film', 'Cast', 'teenage_', 'technical_', 'skip', 'rend', 'our', 'illus', 'ham', 'favourite_', 'ensi', 'consist', 'cold_', 'cent', 'cate', 'MAN', 'F_', 'Die', 'Cub', 'Chinese_', 'yourself', 'ugh', 'stretch', 'society', 'rth', 'root', 'reminds_', 'reg', 'rd_', 'put', 'purpose', 'ition_', 'humanity', 'gotten_', 'fest', 'feel', 'fascinat', 'failure', 'culture_', 'cont', 'allow_', 'pursu', 'preci', 'if', 'belong', 'VE', 'Sar', 'O_', 'Nic', 'Dead', 'AC', ' ****', 'western_', 'uct', 'thro', 'tes', 'struggle_', 'straight', 'stic', 'similar', 'repe', 'pid', 'nes_', 'mou', 'irre', 'hic', 'explained', 'deeply_', 'cs_', 'confront', 'clichés', 'attack_', 'asks_', 'Yet_', 'Was_', 'Tro', 'Stre', 'Rei', 'Kelly_', 'Julia', 'Bas', '? <', 'ties', 'technique', 'stunning_', 'slight', 'skill', 'sat_', 'outstanding_', 'lies_', 'journey_', 'hap', 'expla', 'definit', 'critics_', 'continue_', 'compelling_', 'charge', 'Thing', 'PE', 'Marie', 'Lynch', 'Jason_', 'Hen', 'Av', '.... ', '\\x97', 'wanting_', 'wanna', 'transp', 'thats_', 'smok', 'respons', 'professional_', 'print', 'physic', 'names_', 'inge', 'infa', 'grip', 'green', 'ggi', 'buster', 'bum', 'belief', 'accept_', 'abuse', 'Rain', 'Pos', 'Lee', 'Hoo', 'All', 'threa', 'soundtrack', 'realized_', 'ration', 'purpose_', 'notice_', 'member_', 'lovers', 'log', 'kni', 'inse', 'inde', 'impl', 'government_', 'door_', 'community', 'also', 'Zombie', 'WI', 'Sur', 'Stewart_', 'Roo', 'NA', 'Comm', 'Anna', 'wonderfully_', 'vac', 'tit', 'thus_', 'shadow', 'rg', 'resol', 'religious_', 'problems', 'nonsense', 'naked_', 'marvel', 'fantastic', 'em_', 'earth_', 'demand', 'cost', 'bes', 'band', 'background', 'Mas', 'Bon', 'African', ':<', 'thousand', 'realism', 'race_', 'ption', 'pred', 'neg', 'met', 'little', 'kn', 'flying_', 'ement', 'editing', 'abandon', 'Take', 'On', 'Mich', 'Gin', 'Fer', 'wide', 'victim_', 'spell', 'search_', 'rush', 'road_', 'rank', 'pping_', 'mpl', 'kil', 'incomp', 'humour_', 'group', 'ghost', 'ens', 'electr', 'edg', 'dru', 'culture', 'cars', 'Wil', 'UR', 'Haw', 'Give', 'Fat', 'Dou', 'Ant', 'AD', 'vs', 'tia', 'rei', 'regret', 'necessar', 'master_', 'mani', 'honestly_', 'hey', 'hadn', 'gant', 'fresh_', 'exce', 'document', 'direct_', 'dated_', 'afraid_', 'OU', 'Mid', 'Len', 'Good', 'Beat', 'yer', 'walk', 'ture_', 'train_', 'theor', 'stink', 'spit', 'rarely_', 'proper', 'intelligen', 'hed_', 'hair_', 'forgot', 'fascinating_', 'ere', 'deliver_', 'believable', 'awesome_', 'attend', 'actresses_', 'Up', 'Par', 'Bad_', 'zombie', 'ys_', 'wards', 'trash_', 'strip', 'spectacular', 'six_', 'silly', 'shed_', 'praise', 'loud_', 'inspir', 'insi', 'god', 'four', 'devi', 'Sir', 'Plan', 'PL', 'Everyone_', 'Dol', 'thinking', 'store', 'spo', 'rou', 'pou', 'opposite', 'dud', 'difference_', 'deli', 'compare_', 'cable', 'VER', 'Tim_', 'Ob', 'Jane', 'Jam', 'Don_', 'CI', 'yo', 'want', 'villains', 'toward_', 'taste_', 'support', 'stone', 'sted_', 'spect', 'satire', 'row_', 'rag', 'observ', 'nel_', 'motiv', 'moro', 'lust', 'lect', 'ively_', 'gli', 'gie', 'fet', 'eld', 'div', 'creating_', 'brain_', 'bird', 'attention', 'ates_', 'ald', 'Sher', 'Russ', 'Rea', 'Joan_', 'Gab', 'Coo', 'Bond', '40', 'trade', 'sive_', 'routine', 'plane_', 'photograph', 'ound', 'om_', 'nk_', 'mountain', 'mate', 'listen_', 'isa', 'imagina', 'gia', 'embarrassing', 'convince', 'building_', 'avoid', 'Wow', 'SA', 'Al_', 'vy_', 'unsu', 'tty_', 'situations_', 'sensi', 'results', 'recogni', 'quick', 'plan_', 'mod', 'masterpiece_', 'limit', 'lar', 'gorgeous_', 'fil', 'ensu', 'edly_', 'cor', 'context', 'bul', 'bottom_', 'began_', 'animation', 'anc', 'acc', 'Ty', 'Sc', 'London_', 'Lewis', '.\"<', 'weight', 'rubbish', 'rab', 'project_', 'powers', 'personalit', 'offer_', 'noir_', 'killed', 'justif', 'jun', 'information_', 'gem', 'ative_', 'PO', 'Jeff_', 'Gui', 'voca', 'tab', 'spot_', 'remind', 'proceed', 'kick_', 'ious', 'grab', 'enem', 'educat', 'claim', 'cks', 'charisma', 'bal', 'Scott', 'Over', 'Mus', 'Laure', 'Kan', 'Hunt', 'Dead_', 'Acti', '90_', '50_', ' ! ! ! ! ! ! ! ! ! !', 'ws_', 'vul', 'village', 'speed', 'skills', 'public', 'outl', 'naive', 'mos', 'latter_', 'ki_', 'iat', 'honest_', 'ga_', 'emotions_', 'detective_', 'citi', 'bits_', 'answer_', 'accomplish', 'Washington', 'Sm', 'Dal', 'CE', 'Bett', 'Af', '40_', 'sell', 'pret', 'pper', 'opera', 'notabl', 'involved', 'important', 'humorous', 'finale', 'dise', 'date_', 'contribut', 'complain', 'comedies_', 'battle', 'balance', 'Go_', 'Fla', 'Alon', '); ', 'wis', 'ups', 'spoke', 'pulled_', 'points', 'mediocre_', 'ker_', 'introduced_', 'independent_', 'hil', 'fits_', 'eating_', 'confused_', 'concerned', 'cing', 'ca_', 'bran', 'borat', 'bing_', 'ay_', 'abr', 'Russian_', 'Kevin_', 'H_', 'Fred_', 'Exce', 'English', 'Danny_', 'Dani', 'Coll', 'Alt', '100_', 'used', 'translat', 'shape', 'odi', 'manage_', 'loy', 'lik', 'ibi', 'eat_', 'behav', 'apparent_', 'admi', 'acr', 'ach', 'Young_', 'Run', 'Martin_', 'Mak', 'Hart', 'Asi', '25', '& ', 'trag', 'terror', 'tea', 'shallow', 'rob', 'rape', 'pond', 'ole', 'neck', 'nature', 'loving_', 'jerk', 'hours', 'hidden_', 'gar_', 'field', 'fel', 'existence', 'erotic', 'constant_', 'cau', 'bar_', 'VI', 'Univers', 'Sen', 'CK', '100', 'wealth', 'wave', 'understanding_', 'sole', 'ral', 'none', 'nasty_', 'mari', 'likable_', 'ith', 'intense_', 'hou', 'gh_', 'ely', 'dic', 'dea', 'clip', 'bow', 'UL', 'Nu', 'Moon', 'Ital', 'Ed_', 'Cle', '.......', 'yeah', 'tree', 'successful', 'ril', 'ract', 'philosoph', 'parents', 'marriage_', 'lte', 'ject', 'ite_', 'hun', 'fantas', 'fame', 'extra_', 'dreadful', 'details_', 'dad_', 'capture_', 'annoy', 'Other', '?!', 'tions', 'stalk', 'speak', 'revolution', 'redu', 'pretend', 'politic', 'places_', 'parody', 'park', 'onic', 'nowhere_', 'mono', 'mile', 'manipulat', 'loses_', 'lli', 'into', 'hid', 'ghost_', 'gha', 'engage', 'assum', 'ador', 'admire', 'X_', 'See_', 'Full', 'Eye', 'zy', 'ware', 'ven_', 'uncle', 'treated_', 'television', 'surreal', 'student_', 'rival', 'ride_', 'recall', 'nudity', 'locations', 'ility', 'hamm', 'gags', 'fill_', 'dealing_', 'co_', 'climax_', 'bon', 'atmosphere', 'aged_', 'Rock_', 'Kim', 'Had', 'Brid', 'Anton', 'zombies_', 'unfunny', 'techn', 'source', 'section', 'pris', 'priest', 'police', 'olo', 'nine', 'maker', 'limited_', 'ik', 'genius_', 'enjoyable', 'distan', 'desperate_', 'believe', 'asked_', 'appearance', 'Ring', 'Pete', 'Master', 'Kin', 'Harr', 'Earth', 'Dog', 'Brown', 'Bren', 'Add', 'web', 'tee', 'sucks', 'structure', 'regi', 'porn_', 'osi', 'llian', 'lett', 'length_', 'ior_', 'hal', 'faith', 'enta', 'deserve_', 'cartoon', 'bs_', 'ahead_', 'Got', 'Eu', 'Americans_', 'Alex', 'speaking_', 'smil', 'photographe', 'ope', 'mpe', 'minim', 'million', 'mental', 'magnificent', 'lur', 'lov', 'keeping_', 'iting', 'homo', 'haunt', 'fiction_', 'fee', 'exploit', 'entertain', 'dding', 'attracti', 'advice', 'Park', 'Fur', 'Cage', 'suc', 'songs', 'smart_', 'shock_', 'rif', 'repl', 'ranc', 'ran', 'photography_', 'patient', 'ladies', 'hated_', 'growing_', 'cheer', 'attractive_', 'ass_', 'approach', 'ants_', 'Mrs', 'Hay', 'Hank', 'Eli', 'EVER', 'Batman_', 'week', 'sword', 'rac', 'promot', 'portray', 'pictures_', 'lt_', 'ito', 'interna', 'forgive', 'device', 'corrupt', 'choreograph', 'chop', 'blame_', 'atch', 'VE_', 'KE', 'Johnny_', 'vity', 'ville', 'vas', 'uit', 'tional_', 'quote', 'quick_', 'producer_', 'personally_', 'parti', 'oa', 'nity', 'loo', 'ives', 'increas', 'ical', 'heads_', 'graphic', 'going', 'featuring_', 'defin', 'cute', 'criminal', 'cheat', 'cash', 'cann', 'bol', 'bec', 'Welles', 'SPOILERS', 'Power', 'Kell', 'Georg', 'Gene_', 'Blai', 'Again', '11', 'yell', 'vious', 'unusual_', 'tradition', 'summar', 'stunn', 'revealed', 'remo', 'psychi', 'provi', 'prepare', 'offer', 'insane', 'happens', 'efforts', 'delic', 'current_', 'construct', 'bil', 'aries', 'animals_', 'advance', 'Kong', 'Jan', 'Howard', 'Daw', 'Cru', ' !', 'terribly_', 'teache', 'tas', 'sudden', 'sleaz', 'sharp', 'ress_', 'rape_', 'ppi', 'numbers_', 'mouth', 'lower', 'ime', 'ifie', 'ideal', 'exception_', 'ema', 'charm_', 'breaking_', 'addition_', 'Walke', 'Lat', 'Jean_', 'Eddie_', 'City_', '.\"', 'warning', 'versions', 'tack', 'reli', 'ration_', 'prove_', 'plo', 'pile', 'performer', 'monk', 'intellectual', 'handle', 'ets', 'essor', 'ature', 'atri', 'ans_', 'Int', 'Fel', 'European_', 'Cus', 'As', 'wr', 'worst', 'witty', 'wild_', 'wedding', 'students_', 'sadly_', 'princip', 'paint', 'mmy', 'mixed_', 'kinda_', 'frequent', 'discover_', 'dal', 'command', 'colour', 'bou', 'bored', 'Wild', 'Ul', 'Really', 'Mitch', 'Cinema', 'Andy_', '16', 'visuals', 'varie', 'ut_', 'unfold', 'suspect', 'semi', 'responsible_', 'religion', 'rapi', 'py_', 'otic', 'numerous_', 'news', 'nces', 'kl', 'junk', 'joy', 'insult', 'festival', 'drop_', 'costumes_', 'been', 'bag', 'aware_', 'aver', 'Mir', 'Last_', 'Hon', 'Frie', 'Cent', 'wishe', 'vie', 'toy', 'repeat', 'pter', 'oppo', 'open', 'noticed_', 'murders_', 'ka_', 'harm', 'finish_', 'extreme_', 'eno', 'dying_', 'doo', 'ddle', 'clear', 'cat_', 'bru', 'addict', 'Smith', 'Rod', 'Rem', 'zzle', 'tory', 'starting_', 'specific', 'screaming', 'scenery_', 'psychological_', 'occur', 'obli', 'mn', 'lica', 'laughter', 'inso', 'grad', 'goof', 'gas', 'element', 'dom_', 'dism', 'deals_', 'ctor', 'camp_', 'audi', 'ator_', 'ack', 'Smith_', 'Sh', 'Kenne', 'Holl', 'Dean', 'xious', 'uncom', 'situation', 'shots', 'seem', 'rin', 'pain_', 'originally_', 'number', 'nightmare', 'mystery', 'ml', 'kiss', 'imag', 'iful', 'grew_', 'grade_', 'gge', 'event', 'eate', 'dramati', 'dad', 'condition', 'conce', 'comfort', 'chair', 'aur', 'YOU', 'Red', 'REAL', 'Norma', 'Kir', 'wash', 'upt', 'titi', 'returns_', 'retr', 'restr', 'require', 'relief', 'realise', 'rch', 'rang', 'ple_', 'lus', 'lip', 'intrigue', 'incident', 'iler', 'ha_', 'ground_', 'fores', 'exh', 'dancer', 'anger', 'Wr', 'They', 'Sinatra', 'SI', 'Op', 'Long', 'GI', 'Dem', 'yd', 'week_', 'treatment', 'treat', 'stan', 'slic', 'separate', 'screenplay', 'remarkable_', 'pped_', 'persona', 'mble', 'invi', 'innocen', 'hack', 'gru', 'gma', 'glass', 'forgotten_', 'fem', 'confi', 'clever', 'bone', 'amateur', 'Richard', 'Ray_', 'Please_', 'Kris', 'IM', 'Gordon', 'ED', 'Black', 'wen', 'very', 'ured', 'theater', 'stab', 'redi', 'perce', 'peace', 'passe', 'ops', 'oon', 'morning', 'llow', 'legend', 'irritating', 'hopes_', 'gross', 'genuinely_', 'ech', 'crus', 'bitter', 'acti', 'accura', 'Yu', 'Rome', 'Parker', 'Dia', 'studio', 'still', 'stereotypes', 'serv', 'sequences', 'sequence', 'pres', 'portray_', 'poet', 'opti', 'only', 'ins_', 'impact_', 'emotion_', 'ek_', 'earth', 'dou', 'dislike', 'Sti', 'Reg', 'Philip', 'Bil', 'Att', 'Ash', 'Adam_', 'viol', 'v_', 'uma', 'ultimate_', 'ught', 'trailer_', 'superior_', 'sucked', 'sno', 'service', 'ride', 'por', 'plan', 'mum', 'mme', 'merc', 'lonel', 'guide', 'fici', 'facts', 'evidence', 'doctor', 'discover', 'depend', 'degree', 'cruel', 'counter', 'color_', 'cess', 'cause', 'bro', 'ambitio', 'amaze', 'alternat', 'Wom', 'White_', 'John', 'Bud', 'wound', 'wander', 'typi', 'technology', 'swe', 'standing_', 'reuni', 'organi', 'ngly_', 'minu', 'leas', 'gift', 'executed', 'environment', 'diss', 'demonstrat', 'compani', 'allows_', 'Wayne', 'Kno', 'Instead', 'DA', 'Cart', 'Anthony_', 'unable_', 'uf', 'twin', 'tely', 'sympathetic', 'spoof', 'sis', 'saying', 'rh', 'repr', 'rave', 'promising', 'nch_', 'moo', 'ming', 'liz', 'lighting_', 'lesbian', 'large', 'izing_', 'impos', 'dor', 'disco', 'corny', 'arts_', 'Wars', 'Trac', 'Seve', 'Poli', 'PA', 'Moore', 'LL_', 'Jimmy_', 'Gary_', '?\"', 'zero', 'underw', 'tou', 'spen', 'sheer_', 'scared_', 'rever', 'relationships_', 'proved_', 'predict', 'pia', 'obsc', 'lum', 'learn', 'herself', 'gras', 'finished_', 'continues_', 'brave', 'aris', 'api', 'THIS_', 'Mille', 'Leg', 'First', 'Dis', 'Allen_', 'traditional_', 'statement', 'spir', 'soon', 'rence', 'ran_', 'pros', 'opi', 'mistake_', 'lawyer', 'discovers_', 'deepe', 'ction_', 'cares', 'brutal_', 'brutal', 'breaks_', 'antly', 'accent_', 'Killer', 'Can_', 'Broadway', 'unintentional', 'unbelievable_', 'tte_', 'suspect_', 'strike', 'sens', 'screw', 'rtu', 'pant', 'opens_', 'obsessi', 'mates', 'los', 'logic', 'kit', 'joy_', 'inte', 'iness_', 'han_', 'exact', 'entertained', 'ego', 'dreams_', 'convention', 'collecti', 'chest', 'bling_', 'authentic', 'Then', 'Much_', 'Mot', 'Bette', 'viewers', 'vampire_', 'teach', 'stylis', 'someone', 'sne', 'saved_', 'rule', 'regular_', 'practic', 'ppe', 'pion', 'notice', 'native', 'monsters', 'lo_', 'learned_', 'incon', 'hour', 'hood_', 'feeling', 'embe', 'driving_', 'convincing', 'cav', 'ber_', 'angle', 'absurd', 'Trek', 'Sat', 'Paris_', 'Mol', 'Max', 'Kh', 'Emma', 'Edward', 'Anyone_', '?? ', '17', ' \" ', 'wrap', 'unrealistic', 'tam', 'subtitle', 'spoilers', 'since', 'sexual', 'render', 'remake', 'rely', 'pop_', 'oge', 'oft', 'nett', 'monst', 'law_', 'ional', 'inclu', 'ich', 'ians_', 'hotel_', 'graphic_', 'gonna_', 'gent', 'flashbacks', 'families', 'erin', 'dropp', 'dir', 'bond', 'affair_', 'Scre', 'Dun', 'wide_', 'ttl', 'topic', 'symboli', 'switch', 'solve', 'send', 'rud', 'rem', 'reasons', 'reasonabl', 'pee', 'nar', 'location_', 'ining_', 'gam', 'disappointing_', 'desire_', 'criminal_', 'considera', 'century_', 'celebrat', 'brow', 'area', 'Thin', 'Rec', \"' (\", 'ward_', 'vision_', 'treme', 'surprising_', 'super_', 'risk', 'receive', 'qual', 'pic', 'mee', 'levels', 'kins', 'jack', 'ire_', 'introduc', 'hits_', 'happening_', 'handsome', 'gradua', 'giv', 'garbage', 'forces_', 'finest_', 'easi', 'depressing', 'credits', 'asto', 'Sadly', 'Ple', 'Inc', 'Dick_', 'Alexand', 'wooden_', 'wood_', 'stro', 'steal_', 'soul_', 'reference', 'race', 'quis', 'pir', 'perv', 'obvious', 'majority_', 'lean', 'kes_', 'insti', 'identity', 'everybody_', 'double_', 'dies', 'credit', 'const', 'confe', 'compar', 'centur', 'bloody_', 'Under', 'Twi', 'Sean_', 'Lio', 'Halloween', 'Gal', 'Clu', 'Came', 'Barbara_', '?)', '11_', 'ws', 'ulous', 'subtle', 'substance', 'string', 'shocking_', 'scientist_', 'rian', 'nou', 'multi', 'lf', 'inal', 'harsh', 'handed', 'fir', 'expectations_', 'excited', 'exceptional', 'eva', 'complete', 'comic', 'childhood_', 'ched_', 'adults_', 'Timo', 'Soo', 'Mos', 'Kath', 'Karl', 'Cinderella', 'Christian', 'Age', 'Adam', '!). ', 'zar', 'zan', 'trap', 'trai', 'thin_', 'site_', 'site', 'rich', 'resi', 'reach_', 'quirk', 'patr', 'ony', 'nerv', 'matche', 'inept', 'imagine', 'horri', 'front', 'ford_', 'epic_', 'dat', 'cynic', 'ckin', 'cie', 'caused_', 'brothers_', 'belo', 'appealing', 'West_', 'UK', 'TC', 'Suc', 'Rand', 'Grad', 'Domin', 'Disney', '12_', 'warr', 'vision', 'spoo', 'seeing', 'scenario', 'scale', 'rad', 'ola', 'next', 'necessary_', 'indicat', 'exploitation', 'ened_', 'directing', 'depict', 'curio', 'ciati', 'bullet', 'appre', 'amateurish', 'Yo', 'Watching_', 'Sky', 'Shar', 'Part_', 'Nichol', 'Mars', 'Are_', 'wel', 'visit_', 'unne', 'underrated', 'tedious', 'seconds_', 'rig', 'report', 'reme', 'rar', 'mond_', 'media_', 'lying_', 'las', 'language', 'ised_', 'instant', 'inspiration', 'creates_', 'conflict', 'compose', 'chan', 'cab', 'ava', 'always', 'Water', 'Steven_', 'Pas', 'Nick_', 'Let_', 'Down', 'yth', 'victims_', 'theaters', 'seasons', 'sai', 'rising', 'recr', 'plann', 'pent', 'painfully_', 'ot_', 'occu', 'nob', 'moti', 'lem', 'lati', 'gua', 'fights_', 'event_', 'elev', 'discovered_', 'cs', 'cliché_', 'cance', 'bik', 'bigger_', 'backs', 'atic', 'Shan', 'Sab', 'Poi', 'Hitchcock', 'GR', 'Francis', 'Det', 'Care', 'Anderson', 'veteran', 'ution_', 'theless', 'sports', 'slave', 'ses', 'revi', 'refreshing', 'quar', 'provok', 'premise', 'paper', 'nty', 'norm', 'mood', 'menac', 'loud', 'loose', 'letter', 'investigati', 'introduce', 'holes_', 'gan_', 'fund', 'ents_', 'drunk', 'disgusting', 'dio', 'confusing_', 'cky', 'baby', 'THE', 'Nancy', 'Kate_', 'Gia', 'Carol', 'Cand', \"'.\", 'western', 'unf', 'struc', 'strong', 'search', 'sav', 'ries_', 'resemble', 'rental', 'raci', 'producer', 'nic_', 'news_', 'memor', 'many', 'magical', 'format', 'equal', 'decl', 'curs', 'ction', 'convict', 'contrived', 'capable_', 'bringing_', 'boyfriend_', 'bli', 'anybody_', 'animal_', 'advertis', 'Music', 'Jun', 'Jones', 'Greg', 'Fra', 'Donald_', 'Dark', '1930', 'é_', 'yc', 'urne', 'tire', 'step', 'scr', 'reporter', 'position', 'okay', 'nted_', 'misse', 'logical', 'ient', 'identif', 'feet', 'fail_', 'creat', 'content_', 'contemp', 'concei', 'border', 'ask', 'actual', 'Way', 'Plus', 'Mill', 'Foo', 'Dy', 'Bec', ' ,', 'utter_', 'urban', 'struggle', 'sign_', 'sher', 'seduc', 'scientist', 'saw', 'released', 'received_', 'lity_', 'jump_', 'island_', 'ignor', 'ick', 'horrifi', 'hange', 'handled', 'endea', 'dil', 'ative', 'angry_', 'ages_', 'accus', 'Writ', 'Without_', 'Wall', 'Thank', 'Sla', 'Qua', 'Page', 'ND', 'Lost', 'Fish', 'Eric_', 'Does', 'Clau', 'Cel', 'Camp', 'Australian', 'Arn', 'Ann_', 'Ala', 'Actually', \".' \", \",' \", 'wall_', 'thoughts', 'somebody_', 'round', 'proud', 'oy', 'overly_', 'opera_', 'offensive', 'myth', 'murderer', 'mpt', 'ivi', 'ir_', 'iga', 'iar', 'holi', 'hearted_', 'gath', 'fictional', 'expectation', 'etta', 'enco', 'ence', 'deserved_', 'depiction', 'dece', 'comedian', 'bles', 'aside_', 'ambi', 'ake', 'Wonder', 'Why', 'Through', 'Overall_', 'Off', 'OI', 'More_', 'Jennifer_', 'Gill', 'Germany', 'Douglas_', 'Cy', 'CGI_', '\").', 'walks_', 'ury', 'three', 'thank_', 'surp', 'soph', 'sed', 'satisfying', 'rebel', 'pure', 'practically_', 'minds', 'manage', 'lp', 'learns_', 'isl', 'involves_', 'impro', 'impa', 'icon', 'hyp', 'fortune', 'erm', 'cuts_', 'copi', 'conclusion_', 'ced_', 'captured_', 'bble', 'arro', 'Wei', 'Sis', 'Pin', 'Marg', 'Life', 'Laur', 'Later', 'Hop', 'Eva', 'Blue', 'Barry', 'Baby', 'whilst_', 'unfa', 'twi', 'test_', 'ters', 'stric', 'streets', 'stom', 'spoil', 'relative', 'relate_', 'recommend', 'ology', 'middle', 'laughable', 'jea', 'genuine_', 'gat', 'frustrati', 'forth', 'excitement', 'costs', 'cord', 'compo', 'bright_', 'bank', 'aka', 'WE', 'Ten', 'THAT', 'Pur', 'Pitt', 'Mike_', 'Hum', 'Being_', 'veri', 'turi', 'tun', 'tel', 'task', 'sting', 'six', 'sentimental', 'quit', 'pleasure_', 'pity', 'personality_', 'motivation', 'moder', 'miserabl', 'mirror', 'manner_', 'logi', 'ein', 'eful', 'dubbed', 'discussi', 'ders', 'defeat', 'dangerous_', 'cry_', 'clos', 'cial_', 'chor', 'Wat', 'Wan', 'Spanish_', 'Have', 'Guy', 'Game', '. . ', 'winner', 'welcome', 'unexp', 'ture', 'tall', 'tal', 'stoo', 'smo', 'serious', 'rc', 'phi', 'outrage', 'oh', 'national_', 'mber_', 'mba', 'loser', 'lee', 'largely_', 'involve', 'ico', 'garbage_', 'found', 'even', 'distinct', 'design_', 'cure', 'consu', 'circumstances', 'calls_', 'blown_', 'attract', 'anime', 'Zi', 'Vietnam', 'Ryan', 'ON_', 'NY', 'Lady_', 'La_', 'Flor', 'Bern', 'AI', ' )', 'unk', 'unh', 'ugly_', 'tine', 'spre', 'simpli', 'significant', 'sequels', 'remembered_', 'reache', 'plat', 'obsessed_', 'ncy_', 'mysteri', 'mous', 'mbs', 'lover_', 'lights', 'lad', 'industr', 'ible', 'grown_', 'general', 'fru', 'explosion', 'exception', 'ese', 'endur', 'domina', 'dera', 'cies', 'built_', 'barr', 'Tod', 'Ran', 'Maria', 'Grand', 'Dee', 'Aw', ' />**', 'xo', 'voices', 'visually', 'ui', 'twice_', 'tend_', 'spor', 'solut', 'slap', 'scien', 'robbe', 'redibl', 'prot', 'prevent', 'ood', 'kee', 'issue_', 'ironic', 'iron', 'investigat', 'intr', 'hl', 'gus', 'food_', 'enl', 'dl', 'described_', 'complaint', 'careful', 'apartment_', 'alcohol', 'aid', 'acy', 'Year', 'Vis', 'Vir', 'Tow', 'Fly', 'Dream', 'Award', '*****', 'vague', 'strat', 'reviewers_', 'offend', 'locat', 'iu', 'ital', 'iev', 'hospital_', 'fou', 'financ', 'filmmaker_', 'farm', 'evening', 'essentially_', 'energy_', 'ef_', 'complex', 'competi', 'ching', 'bal_', 'ax', 'ances', 'acted', 'ace_', 'Story', 'LD', 'Inde', 'Hope', 'Duk', 'Dian', 'Bob', 'Back', 'Any_', 'About_', ' ...', 'yard', 'whenever_', 'wake', 'ures_', 'unse', 'trust_', 'treat_', 'teenager', 'stock_', 'rri', 'rise_', 'rant', 'pupp', 'pte', 'pes', 'overd', 'operati', 'occasional', 'nicely_', 'nical', 'liners', 'impo', 'holding_', 'engaging_', 'diver', 'distribut', 'dim', 'delightful_', 'crappy_', 'cook', 'connection_', 'cohe', 'bore', 'Vincen', 'Susan', 'Rep', 'Powell', 'Oliver', 'Neil', 'Murphy', 'Mic', 'Indi', 'Ele', 'Bru', 'Beaut', '. *', ' />*', 'zation', 'urge', 'urag', 'teenagers', 'seven_', 'river', 'prep', 'nail', 'mble_', 'matters', 'loose_', 'iva', 'issue', 'intriguing_', 'ili', 'god_', 'glimpse', 'ently', 'els_', 'een_', 'develop_', 'desire', 'cops_', 'contra', 'buil', 'broke', 'ater', 'asleep', 'adventur', 'Williams_', 'Wend', 'None_', 'Mod', 'House', 'Horror_', 'Anim', '192', 'ughter', 'trial', 'soap_', 'severe', 'road', 'poster', 'portraying_', 'phr', 'pathetic', 'overlook', 'moving', 'month', 'lau', 'lacking_', 'knowledge_', 'kidnapp', 'interpretation', 'industry_', 'hurt', 'heavi', 'genius', 'false', 'existent', 'execution', 'drop', 'difference', 'determine', 'detail_', 'dent', 'cutting', 'combin', 'comb', 'cket', 'chron', 'capital', 'bodies', 'bic', 'believes_', 'area_', 'angles', 'Ted', 'Sop', 'End', 'Dre', 'Dick', 'Ak', 'Africa', ' ? ', 'vol', 'system', 'steps', 'situations', 'sexuality', 'sets', 'ripp', 'revel', 'rel', 'realiz', 'private', 'paper_', 'notch', 'nge_', 'mistr', 'merit', 'mbl', 'match', 'losing_', 'lme', 'interacti', 'indeed', 'ifica', 'henc', 'heaven', 'fro', 'fon', 'femin', 'faces_', 'enh', 'driven_', 'dressed_', 'dne', 'decen', 'ctic', 'coming', 'club_', 'castle', 'captures_', 'building', 'atic_', 'athe', 'assassin', 'army_', 'alien_', 'abso', 'Tho', 'Scr', 'Prob', 'Para', 'Gor', 'Eg', 'Com', 'City', 'At', 'Apparently', ' / ', 'ule', 'ue_', 'tograph', 'thirt', 'thank', 'suit_', 'suffering_', 'sight_', 'sey', 'screenwriter', 'rell', 'ppet', 'passed_', 'pacing_', 'normally_', 'mill', 'lyn', 'ition', 'gers', 'football', 'faithful', 'expose', 'expos', 'emerge', 'ell_', 'depicted', 'crude', 'criticism', 'combination_', 'claim_', 'carr', 'bt', 'brilliantly_', 'boss', 'analy', 'ame', 'Ray', 'Pic', 'Lord_', 'Kill', 'Fea', 'Evil', 'Bos', 'BS', 'AB', '\" - ', ' :', 'tta', 'trailer', 'soli', 'rum', 'revolve', 'ressi', 'quiet_', 'portrays_', 'populat', 'plant', 'oin', 'occasionally_', 'nost', 'nau', 'mun', 'lb', 'ipat', 'hysteri', 'grow_', 'gag', 'fus', 'foot_', 'finger', 'figur', 'esp', 'equi', 'ener', 'dec', 'chain', 'broken_', 'agent', 'actions_', 'aa', 'Russell', 'Indian', 'Heav', 'Daniel_', 'Ast', ' /> ', 'zard', 'unlikely', 'ump', 'tele', 'teacher_', 'subplot', 'rub', 'rte', 'rly_', 'radio_', 'quir', 'pair_', 'ordinary_', 'oppos', 'nsi', 'mouth_', 'maintain', 'lve', 'loc', 'inventi', 'inexp', 'imitat', 'generate', 'gal_', 'frightening', 'frig', 'foreign_', 'filmmaker', 'excess', 'elle', 'creator', 'count_', 'controvers', 'cliche', 'casti', 'bet_', 'aking_', 'acqu', 'Three', 'Texas', 'Tarzan_', 'Earth_', 'Dan_', 'Besides', 'yw', 'woods_', 'wan', 'vest', 'uous', 'unit', 'therefore_', 'tears_', 'surface', 'steals_', 'sni', 'shut', 'roman', 'roll_', 'rele', 'reaction', 'qualities', 'proper_', 'profession', 'photo', 'months_', 'mem', 'makeup', 'longe', 'lam', 'ix', 'insist', 'inher', 'fying_', 'forgettable', 'faced', 'expens', 'enthusias', 'describ', 'cry', 'commentary_', 'collection_', 'civili', 'category', 'cam', 'believed', 'ancient_', 'Walter_', 'Sum', 'Sometimes', 'Sel', 'Lou', 'Kn', 'Joseph_', 'Gro', 'Fon', 'Columbo', 'system_', 'student', 'shocked', 'sell_', 'ridi', 'prior', 'primar', 'mon_', 'mmer', 'lish', 'higher_', 'fatal', 'employe', 'dirty', 'cris', 'conf', 'ckle', 'blend', 'bility_', 'baseball', 'awake', 'arr', 'ape', 'alive_', 'Wid', 'Santa_', 'Kei', 'Dep', 'Burn', 'Bob_', '´', 'warn', 'unknown_', 'twenty_', 'touches', 'supernatural', 'sitcom', 'saving_', 'rupt', 'relatively_', 'possibilit', 'nose', 'mes_', 'massive', 'male', 'ied', 'honor', 'heroes_', 'gig', 'gangs', 'divi', 'diat', 'consequen', 'classics', 'cases', 'bug', 'brief', 'bott', 'assume_', 'associate', 'assistan', 'arra', 'aria', 'absen', 'VHS_', 'Steve', 'Port', 'Paris', 'Old_', 'Morgan_', 'Horr', 'High_', 'General', 'Din', 'Dark_', 'Colo', 'Avoid_', 'zel', 'unnecessary_', 'unexpected_', 'tragedy_', 'tim', 'stle', 'stereo', 'stai', 'send_', 'recommended_', 'produce', 'pregnan', 'noon', 'move', 'ludicrous', 'lude', 'length', 'ident_', 'ide_', 'grue', 'focused', 'extraordinar', 'desperate', 'depress', 'dai', 'creature_', 'covered_', 'chief', 'boss_', 'asking_', 'Yeah', 'WW', 'Rid', 'Island', 'FA', 'Denn', 'Ch', 'Basically', 'Ang', 'Ami', '?! ', '): ', 'virtually_', 'underg', 'truck', 'training', 'tif', 'surf', 'rmin', 'reject', 'rante', 'plots_', 'placed_', 'ni_', 'mature', 'lousy_', 'justice_', 'io_', 'glori', 'gentle', 'fly_', 'explanation_', 'execut', 'exaggerat', 'events', 'elie', 'destructi', 'choose_', 'characteriz', 'char', 'cent_', 'books', 'bby', 'appreciated', 'allo', 'Neve', 'Nee', 'Jackson_', 'Irish', 'IN_', 'During_', 'Devil', 'Count', 'yes_', 'user', 'unpr', 'tual', 'treasure', 'stronge', 'sorr', 'ruined_', 'reputation', 'rently', 'related', 'quel', 'produce_', 'presum', 'politics', 'plans', 'painting', 'killers', 'initial_', 'impli', 'ify', 'hooke', 'funnie', 'fad', 'empty_', 'driver', 'di_', 'detect', 'designed', 'deserve', 'believ', 'awesome', 'accents', 'Your', 'Thank_', 'RE_', 'Pacino', 'Movies', 'Jay', 'IMDb', 'Hugh', 'Festival', 'Enter', 'Donn', 'Christi', 'Alm', 'Academy_', '000_', 'ycl', 'vivi', 'upset', 'ups_', 'unp', 'tiny', 'surprises', 'study_', 'strongly_', 'speaks', 'size', 'riv', 'relation', 'quee', 'py', 'never', 'mainstream', 'libera', 'latest', 'ising', 'insu', 'icia', 'hurt_', 'freedom', 'estl', 'emotionally_', 'dust', 'desc', 'convinced_', 'compell', 'cock', 'clothes_', 'cameo_', 'blind_', 'besides', 'attacke', 'Victor_', 'Return', 'Poo', 'Never_', 'Nel', 'Hey', 'Caine', 'Brando', 'ually_', 'tive', 'silen', 'rew', 'quate', 'preach', 'ological', 'nude', 'multiple', 'link', 'lge', 'ledge', 'laz', 'integr', 'hn', 'hie', 'folks_', 'experiences', 'emphasi', 'earlier', 'delivered_', 'deco', 'deaths', 'continuity', 'complicate', 'burne', 'boyfriend', 'awkward_', 'atrocious', 'amuse', 'ack_', 'Wilson', 'Turn', 'Robin_', 'Pr', 'Om', 'Mun', 'Meanwhile', 'Jessi', 'Jess', 'Jenn', 'Gand', 'Et', 'Canadian_', 'Brothers', 'Bake', 'Ah', '1990', 'wreck', 'unif', 'toi', 'teens', 'smart', 'shir', 'serves_', 'sati', 'rix', 'remain_', 'pub', 'propaganda', 'players_', 'plas', 'ping', 'overcom', 'orious', 'minde', 'meeting_', 'lph', 'loyal', 'lm', 'llin', 'lake', 'kar', 'istic', 'instru', 'included_', 'hire', 'graph', 'gory_', 'favour', 'elde', 'dum', 'destroy_', 'destin', 'denti', 'consistent', 'cameo', 'betr', 'arrest', 'appea', 'animal', 'amen', 'accidentally', 'acce', 'Silv', 'Saturday_', 'ST_', 'Res', 'MGM', 'Korea', 'Fam', 'Asian_', 'Alle', 'zu', 'weeks', 'ticke', 'terrifi', 'table_', 'storytell', 'stopped_', 'steal', 'slash', 'shoe', 'select', 'rocke', 'roa', 'record_', 'previously', 'participa', 'okay_', 'ogr', 'official', 'nke', 'mistakes', 'misca', 'memorabl', 'logue', 'itat', 'ists_', 'intelligence_', 'ien', 'greate', 'ggy', 'gangster_', 'critical', 'closer', 'cartoons', 'boot', 'accepta', 'abu', 'TER', 'States', 'Roberts', 'LER', 'Jones_', 'Hat', 'Eri', 'Eliza', 'Coop', 'wes', 'uninteresting', 'tense', 'teet', 'suffers_', 'stranger', 'station_', 'scu', 'resid', 'rand', 'popula', 'ours', 'opene', 'occurr', 'non_', 'nominated_', 'mol', 'missi', 'memory_', 'memories_', 'maid', 'intri', 'inju', 'inevitabl', 'humans_', 'hanging_', 'gratuitous_', 'gas_', 'forme', 'direct', 'difficult', 'department', 'damag', 'creatures', 'cif', 'Warner', 'Titan', 'Matt_', 'Larr', 'KI', 'Hor', 'Holm', 'Fair', 'Drew', 'Andr', '1960', 'wri', 'vely', 'uls', 'travel_', 'trat', 'transf', 'timi', 'suspen', 'struggling', 'spoil_', 'slaps', 'sink', 'reti', 'reaction_', 'quest_', 'pilot_', 'narration', 'invite', 'hearing_', 'gm', 'gai', 'full', 'frankly', 'fairy', 'expe', 'dimension', 'dent_', 'deme', 'contest', 'conscious', 'cked', 'below_', 'ations', 'angel', 'alive', 'absurd_', 'Wer', 'Tha', 'Stewar', 'Play', 'Picture', 'Part', 'Martin', 'Franc', 'Fir', 'Fas', 'Ev', 'Cos', 'Carre', 'Bog', 'BU', 'Anne_', 'yan', 'writ', 'vit', 'vai', 'summ', 'ston', 'stin', 'stif', 'sensitive', 'rules', 'provided_', 'prostitut', 'pretentious_', 'poignan', 'pai', 'paced_', 'offi', 'nds_', 'mig', 'laughable_', 'instal', 'inati', 'forget', 'eit', 'defend', 'conse', 'beaut', 'Spr', 'Rol', 'Our_', 'NOT', 'Lugosi', 'Luci', 'Las', 'Imp', 'Ic', 'Earl', 'Davis_', 'Cod', '!)', 'twiste', 'sincer', 'sacrifice', 'references_', 'range_', 'purchase', 'orn', 'noise', 'neo', 'mecha', 'lun', 'insult_', 'fully', 'flicks_', 'fair', 'endless_', 'eeri', 'devot', 'curious_', 'comical', 'beth_', 'begin', 'aura', 'ase_', 'ach_', 'Sullivan', 'St', 'Sarah', 'London', 'Liv', 'Kee', 'Jackie_', 'Hong', 'Emil', 'Clair', 'China', 'California', 'Atlant', 'Alice', '\"?', '!!!!!!', 'xico', 'wick', 'visi', 'viewed_', 'uish', 'tribu', 'theatrical_', 'talks_', 'smile_', 'seven', 'reminisce', 'relie', 'rci', 'rah', 'pleasant_', 'plague', 'picio', 'ounce', 'murdered_', 'mul', 'mous_', 'mock', 'mira', 'mete', 'loss_', 'initia', 'iest_', 'health', 'harde', 'gran', 'goal', 'ghe', 'fy', 'fix', 'experienced', 'edy', 'deci', 'conflict_', 'compe', 'committed', 'cele', 'brick', 'bour', 'bers', 'berate', 'artist_', 'anth', 'Woody_', 'WWI', 'V_', 'TT', 'Sunday', 'Story_', 'Rob_', 'Rachel', 'Nin', 'Gree', 'Friday', 'Dev', 'Bros', 'Brana', ' : ', 'wha', 'vig', 'views', 'unconvincing', 'smi', 'sibl', 'quen', 'pointless', 'perp', 'particular', 'overwhelm', 'offered', 'nominat', 'naturally', 'locke', 'left', 'lady', 'ilt', 'iel', 'ication', 'historic', 'haunting', 'gem_', 'figures', 'figured_', 'evol', 'ery', 'eco', 'dynami', 'duct', 'doi', 'description', 'cultural', 'contrac', 'confide', 'combined', 'coin', 'cke', 'chosen_', 'amed', 'agon', 'Thomas_', 'THI', 'Nation', 'MOVIE', 'Lev', 'Jeff', 'Hoffman', 'Glen', 'Even', '1st_', ' ! ', 'yu', 'trappe', 'thir', 'tension', 'tail', 'table', 'split', 'sides', 'settle', 'schem', 'save', 'ruc', 'prime', 'posit', 'painte', 'ndi', 'marry_', 'kun', 'killing', 'isol', 'iot', 'intend', 'impres', 'horribly_', 'hing', 'heroi', 'gle_', 'fri', 'fitt', 'fighter', 'estin', 'ee_', 'drunk_', 'directly', 'dinos', 'chose_', 'changing', 'blonde_', 'benefi', 'award_', 'av', 'aki', 'ages', 'acter', 'VERY_', 'Ur', 'Tel', 'Superman_', 'Real', 'Phi', 'Palm', 'Nicol', 'Johnson', 'Jesus_', 'J_', 'Hes', 'Helen', 'Fun', 'Fle', 'Dir', 'Chap', 'vag', 'uncon', 'ues', 'types_', 'tical', 'sprin', 'sorts', 'securi', 'previ', 'porno', 'party', 'pare', 'method', 'medica', 'mber', 'landscape', 'jor', 'jail', 'imper', 'hunter', 'happening', 'gritty', 'gain_', 'flaws_', 'fak', 'extra', 'edited_', 'ecc', 'dragg', 'chie', 'cant_', 'breast', 'authorit', 'ated', 'ality', 'advise', 'advan', 'according_', 'Wors', 'Unlike', 'United_', 'Simon_', 'Riv', 'Pea', 'Michell', 'Exp', 'Child', 'Cham', 'Bourne', 'Basi', 'widow', 'walked_', 'upp', 'unforg', 'uld_', 'tting', 'till_', 'thy_', 'talents_', 'suspenseful', 'summer_', 'storm', 'screening', 'scare_', 'realizes_', 'rce', 'raw', 'qu', 'ngl', 'magic', 'lac', 'jobs', 'ister_', 'inti', 'inha', 'ill_', 'hands', 'grin', 'forward', 'examin', 'equent', 'emi', 'contact', 'concentrat', 'compu', 'competen', 'biograph', 'attach', 'amus', 'alik', 'activi', 'William', 'Myst', 'Luke_', 'Live', 'Life_', '15', 'zes', 'werewolf', 'warne', 'uring_', 'trilogy', 'swim', 'stumble', 'spite', 'spends_', 'sleep_', 'sist', 'sentence', 'rma', 'reward', 'reviewer_', 'pul', 'preten', 'performed', 'passing', 'par_', 'oph', 'livi', 'kinds_', 'journal', 'isticat', 'inva', 'idi', 'ham_', 'fte', 'few', 'featured', 'ern_', 'eag', 'dollars', 'disb', 'depth', 'cryin', 'cross_', 'content', 'contemporary_', 'colors', 'chee', 'because', 'asy', 'agent_', 'Willi', 'Warr', 'Ven', 'Vamp', 'Roch', 'ONE', 'Movie', 'Mau', 'Mass', 'MST', 'Hin', 'Hear', 'Gue', 'Gl', 'Freddy_', 'Definite', 'Captain_', 'BBC', '??? ', '80s_', '\"), ', 'wol', 'weekend', 'vampires', 'underst', 'tial_', 'terrorist', 'strength_', 'starre', 'soldier_', 'snow', 'sity', 'ruin_', 'retar', 'resu', 'required', 'recommended', 'ques', 'propo', 'presents_', 'perm', 'overt', 'olds', 'occas', 'nn_', 'nen', 'nei', 'mail', 'lost', 'lion', 'libr', 'inner_', 'headed', 'happy', 'guest', 'govern', 'friendly', 'explains', 'ens_', 'effectively', 'draw_', 'downright', 'dete', 'dde', 'dare', 'cring', 'courag', 'conspi', 'comedie', 'claims_', 'cide', 'chas', 'captivat', 'bite', 'bare', 'author_', 'addition', 'Vid', 'Rh', 'Oliv', 'Nata', 'Mexican', 'Keaton_', 'Iron', 'Barb', 'ALL_', '12', '!), ', 'worthwhile', 'weake', 'ung', 'understood_', 'unbelievable', 'superf', 'stolen', 'stereotypic', 'spoiler', 'sight', 'scares', 'rut', 'remove', 'remotely_', 'releva', 'prese', 'poke', 'ndou', 'mbla', 'lucky_', 'lling_', 'legendary', 'imagery', 'humou', 'hug', 'hired', 'heck', 'guilty', 'extras', 'expected', 'everywhere', 'dry_', 'drea', 'directed', 'dimensional_', 'ddi', 'dden', 'communica', 'cham', 'buddy', 'bank_', 'azi', 'algi', 'adventures', 'accurate_', 'accompan', 'Thom', 'Still_', 'Someone', 'Serious', 'SU', 'Phill', 'Perso', 'Patrick_', 'Lei', 'Jus', 'Gho', 'Get_', 'Freeman', 'Especially_', '?).', '...\"']\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = imdb['train'], imdb['test']\n",
    "tokenizer = info.features['text'].encoder\n",
    "print(tokenizer.subwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string is [6307, 2327, 7961, 4043, 2120, 2, 48, 2715, 7, 2652, 8050]\n",
      "Original string is Tensor Flow, from basic to mastery\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'Tensor Flow, from basic to mastery'\n",
    "\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print(\"Tokenized string is {}\".format(tokenized_string))\n",
    "\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print(\"Original string is {}\".format(original_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = train_data.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(train_dataset))\n",
    "test_dataset = test_data.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 16)          130960    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_3 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 102       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 131,069\n",
      "Trainable params: 131,069\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embdding_dim = 64\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(tokenizer.vocab_size, embedding_dim),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None,), ()), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /home/reisei88/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /home/reisei88/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/reisei88/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/reisei88/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/reisei88/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:533 train_step  **\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /home/reisei88/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /home/reisei88/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    /home/reisei88/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    /home/reisei88/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:1595 binary_crossentropy\n        K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n    /home/reisei88/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:4692 binary_crossentropy\n        return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n    /home/reisei88/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:172 sigmoid_cross_entropy_with_logits\n        (logits.get_shape(), labels.get_shape()))\n\n    ValueError: logits and labels must have the same shape ((None, 1) vs ())\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-33d959d8d087>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    505\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 506\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /home/reisei88/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /home/reisei88/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/reisei88/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/reisei88/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/reisei88/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:533 train_step  **\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /home/reisei88/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /home/reisei88/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    /home/reisei88/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    /home/reisei88/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:1595 binary_crossentropy\n        K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n    /home/reisei88/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:4692 binary_crossentropy\n        return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n    /home/reisei88/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:172 sigmoid_cross_entropy_with_logits\n        (logits.get_shape(), labels.get_shape()))\n\n    ValueError: logits and labels must have the same shape ((None, 1) vs ())\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(train_data, epochs=num_epochs, validation_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'orange'> API Spec </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizer**\n",
    "\n",
    "<font color ='red'> Tensorflow의 text preprocessing에 있음 </font>\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text\n",
    "\n",
    "Tokenizer: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "\n",
    "text to word sequence: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/text_to_word_sequence\n",
    "\n",
    "**Embedding**\n",
    "\n",
    "<font color ='red'> Tensorflow의 Layer에 있음. </font>\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
    "\n",
    "Embedding : https://www.tensorflow.org/tutorials/text/word_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding 이란?\n",
    "\n",
    "- 범주형 데이터를 numeric Vector로 나타내는 것. 이렇게 숫여 벡터로 나타내면 좌표 평면에도 그릴 수 있고, 연산하기 쉽다. \n",
    "\n",
    "https://developers.google.com/machine-learning/glossary#embeddings\n",
    "\n",
    "https://developers.google.com/machine-learning/recommendation/overview/terminology\n",
    "\n",
    "https://heung-bae-lee.github.io/2020/01/16/NLP_01/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
