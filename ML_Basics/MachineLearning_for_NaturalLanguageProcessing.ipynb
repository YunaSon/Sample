{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords\n",
    "\n",
    "- Preprocessing text data into useful representations\n",
    "- Working with recurrent neural networks\n",
    "- using 1D convnets for sequence processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Data**\n",
    "\n",
    "- Text is one of the most widespread forms of sequence data.\n",
    "- It can be understood as either a sequence of characters or a sequence of words.\n",
    "\n",
    "\n",
    "**Approach**\n",
    "\n",
    "- Deep learning for natural-language processing is pattern recognition applied to words, sentences, and paragraphs, in much the same way that computer vision is pattern recognition applied to pixels. \n",
    "\n",
    "- Like all other neural networks, deep learning models don't take as input raw text: they only work with numeric tensor.\n",
    "- vectorizaing text is the process of transforming text intu numeric tensors.\n",
    "\n",
    "**Token** (1번코드)\n",
    "\n",
    "- <font color ='red'> [Word]Segment text into words and transform each word into a vector </font>\n",
    "- [characters]Segment text into characters, and transform each character into a vector\n",
    "- [n grams] Extract n-grams of words or characters, and transform each n-gram into a vector. N-grams are overlapping groups of multiple consecutive words or characters.\n",
    "\n",
    "**from token to (numeric) vector**\n",
    "\n",
    "- one hot encoding (2번 코드)\n",
    "    - sparse\n",
    "    - High dimensional\n",
    "    - Hardcoded (manual)\n",
    "\n",
    "\n",
    "- <font color='red'> (word) Embedding  or word vector</font> (3번 코드)\n",
    "    - Dense\n",
    "    - Low dimensional\n",
    "    - Learned from data\n",
    "    \n",
    "**Using Embedding, we can start learn with neural network**\n",
    "\n",
    "- 현재.. 코드 설명만..\n",
    "- \n",
    "\n",
    "**LSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='orange'> 1번 코드 - Tokenization </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x7fc67f992790>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['I love my dog', 'Do you love your dog?']\n",
    "tokenizer = Tokenizer(num_words=1000) # 가장 빈도가 높은 1000개의 단어만을 선택하도록 하는 Tokenizer객체를 만든다. \n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], []]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer.fit_on_texts(samples)  # 단어 인덱스를 구축 \n",
    "sequences = tokenizer.texts_to_sequences(samples) # 문자열 정수 인덱스의 리스트로 변환한다. \n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 1, 4, 2], [5, 6, 1, 7, 2]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(samples)  # 단어 인덱스를 구축 \n",
    "sequences = tokenizer.texts_to_sequences(samples) # 문자열 정수 인덱스의 리스트로 변환한다. \n",
    "sequences\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 1, 'dog': 2, 'i': 3, 'my': 4, 'do': 5, 'you': 6, 'your': 7}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='orange'> 2) One-hot encoding </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "one_hot_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='orange'> 3) word embedding </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.embeddings.Embedding at 0x7fc660b751d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(1000, 64) #최소 2개의 인자를 갖는다: 가능한 토큰의 갯수, 임베딩 차원\n",
    "embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 64), dtype=float32, numpy=\n",
       "array([[ 0.02937875, -0.02164763, -0.04868446, -0.04500158,  0.01340489,\n",
       "        -0.03949068,  0.04113687,  0.00587994,  0.0398984 , -0.0334085 ,\n",
       "         0.02137026, -0.03101782, -0.04906091,  0.02629887,  0.01119486,\n",
       "         0.03365156,  0.02182782,  0.00280169, -0.01179569,  0.03081283,\n",
       "        -0.04547173, -0.02078052, -0.00947034,  0.04407955, -0.00462257,\n",
       "        -0.00358478,  0.04757723,  0.04679709, -0.04395753, -0.02391377,\n",
       "         0.00961566,  0.00706487,  0.04960166,  0.00042254, -0.01825488,\n",
       "         0.01248141,  0.02371858,  0.02403371, -0.04207597,  0.04734487,\n",
       "        -0.01319177, -0.01313963,  0.04051982,  0.02816084, -0.03024213,\n",
       "        -0.03240237, -0.00022344,  0.01583363, -0.04344045,  0.03355196,\n",
       "        -0.04172082,  0.03851492,  0.01236711, -0.01515079,  0.04749196,\n",
       "         0.04824683, -0.01755846,  0.02729542, -0.00118617,  0.01037337,\n",
       "         0.0166187 ,  0.03940277,  0.02989114, -0.01553128],\n",
       "       [-0.02419022,  0.01029623, -0.04521323,  0.02772382,  0.0301897 ,\n",
       "        -0.01800685,  0.03962392,  0.04610859, -0.02289246,  0.03135773,\n",
       "        -0.03211148,  0.04991907, -0.02859258,  0.00353439,  0.01131896,\n",
       "         0.04466167, -0.02834917, -0.02284755,  0.02803707, -0.01394913,\n",
       "        -0.03474478, -0.04669946, -0.03749578,  0.00733925,  0.00420361,\n",
       "         0.01397187,  0.02794919, -0.02786616, -0.00197476, -0.00120245,\n",
       "         0.02662288, -0.01255611, -0.02973058,  0.02038855,  0.00644913,\n",
       "        -0.03058939, -0.03791821, -0.03084434, -0.01269187, -0.04460546,\n",
       "         0.0316355 ,  0.0319482 , -0.01751826,  0.0240914 ,  0.01567105,\n",
       "         0.03292611,  0.02614534, -0.03462279, -0.00203764, -0.01193672,\n",
       "        -0.04654081,  0.00104468,  0.02718189,  0.02953067,  0.03877754,\n",
       "        -0.04953181, -0.026024  , -0.01989296,  0.02329635,  0.03452562,\n",
       "         0.00771338,  0.03235627, -0.00618003, -0.03374521],\n",
       "       [-0.02501279,  0.00436543,  0.04251143,  0.01384756, -0.04909637,\n",
       "         0.03069306,  0.0162181 ,  0.00873924, -0.01693777,  0.0402879 ,\n",
       "         0.03625867, -0.04111652, -0.01540459, -0.01075168,  0.00231805,\n",
       "        -0.04164732, -0.02463026, -0.02738743,  0.01910971,  0.02702768,\n",
       "        -0.02606586,  0.02112833,  0.02780637, -0.04971858, -0.01236717,\n",
       "         0.04289499, -0.00756075,  0.03696448, -0.04696882, -0.02656233,\n",
       "        -0.01538783,  0.03041616, -0.02343108,  0.008372  , -0.04537535,\n",
       "        -0.03429029,  0.01410543, -0.02439547, -0.00071154,  0.01320528,\n",
       "         0.04908342,  0.04135314, -0.01695969,  0.02444453, -0.02082199,\n",
       "         0.01561191, -0.01092048, -0.01059393, -0.00358895, -0.03943062,\n",
       "        -0.010602  , -0.00366513, -0.02108107, -0.01921588, -0.02664768,\n",
       "        -0.0283209 , -0.00909288, -0.04145923,  0.04140724,  0.0123288 ,\n",
       "        -0.03205865,  0.01251448, -0.02810761, -0.00568758]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "example = embedding_layer(tf.constant([1,2,3]))\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.numpy().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding 이란?\n",
    "\n",
    "https://developers.google.com/machine-learning/glossary#embeddings\n",
    "    \n",
    "https://developers.google.com/machine-learning/recommendation/overview/terminology    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://heung-bae-lee.github.io/2020/01/16/NLP_01/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'> 1. Tokenization - Getting your text ready </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font> Tokenization </font>\n",
    "\n",
    "- how to represent words in a way that a computer can process them. \n",
    "- with view to later training a neural network that can understand their meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font> Word(letter) : Number(Encoding), Sequence </font>\n",
    "\n",
    "\n",
    "- the word for example \"listen\" is represented by number using an encoding scheme (ASCII CODE... etc..)\n",
    "- But the order of number is important\n",
    " \n",
    "`listen = [76, 73, 83, 84, 69, 78]`\n",
    "\n",
    "`silent = [83, 73, 76, 69, 78, 84]`\n",
    "\n",
    "- this bunch of numbers can then represent the word listen but word silent has the same letters, and thus the same numbers, just in a different order.\n",
    "- It makes it hard for us to understand sentiment of a word just by the letters in it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font > Sentence : Tokenization </font>\n",
    "\n",
    "- So it might be easier, instead of encoding letters to encode words. \n",
    "- Consider the sentence I love my dog.\n",
    "- what would happen if we start encoding the words in this sentence instead of letters in each word?\n",
    "\n",
    "`sentence1 = {'I':1, \"love\":2, \"my\":3, \"dog\":4}`\n",
    "\n",
    "`sentence2 = {'I':1, \"love\":2, \"my\":3, \"cat\":5}`\n",
    "\n",
    "- two sentences are already show some form of similarity between them.\n",
    "- And it's a similarity you would expect, because they're both about loving a pet.\n",
    "- Given this method of encoding sentences into number, now let's kate a look at some code to archieve this for us. : This process, is called tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font> Tensorflow Tools : Tokenizer </font>\n",
    "\n",
    "- You will see how words can be tokenized and tools in tensorflow that handle that tokenization for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"
     ]
    }
   ],
   "source": [
    "sentence = ['I love my dog',\n",
    "            'I love my cat']\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_texts(sentence)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
     ]
    }
   ],
   "source": [
    "sentence.append('you love my dog!')\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_texts(sentence)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that your words are represented by numbers like this, \n",
    "- you'll next need to represent your sentences by sequences of numbers in the correct order\n",
    "- You'll then have data ready for processing by a neural network to understand or maybe oeven generate new text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequencing - Turning sentences into data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- creating sequences of numbers from your sentences\n",
    "- And using tools to process them to make them ready for teaching neural network\n",
    "\n",
    "- Last time, we saw how to take a set of sentences and use the tokenizer to turn the words into numberic tokens.\n",
    "- Let's build on that now by also seeing how the senteces containing these words.\n",
    "- Can be turned into sequences of numbers.\n",
    "- We'll add another sentence to our set of texts, and I'm doing this because the existing sentences all have four words\n",
    "- and it's important to see how to manage sentences, or sequences, of different lengths\n",
    "- The tokenizer supports a method called texts to sequences which performs most of the work for you.\n",
    "- It creates sequences of tokens representing each sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'my': 1,\n",
       " 'love': 2,\n",
       " 'dog': 3,\n",
       " 'i': 4,\n",
       " 'you': 5,\n",
       " 'cat': 6,\n",
       " 'do': 7,\n",
       " 'think': 8,\n",
       " 'is': 9,\n",
       " 'amazing': 10}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = ['I love my dog',\n",
    "            'I love my cat',\n",
    "            'You love my dog!',\n",
    "            'Do you think my dog is amazing?']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100)\n",
    "tokenizer.fit_on_texts(sentence)\n",
    "word_index = tokenizer.word_index\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- word_index : you can see the list of word-value pairs for the tokens.\n",
    "- sequences : you can see that the sequences that texts to sequences has returned.\n",
    "- We have a few new words such as amazing, think is and do, \n",
    "    - that's why this index looks a little different than before.\n",
    "    - And now, we have the sequences. \n",
    "\n",
    "- 4,2,1,3: tokens for I, love, my, dog\n",
    "- 4,2,1,6: tokens for I, love, my, cat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic tokenization done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this is all very well for getting data ready for training a neural network, \n",
    "- but what happens when that neural network needs to classify texts, but there are words in the text that it has never seen before?\n",
    "- This can confuse the tokenizer, so we'll look at how to handle that next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 2, 1, 3], [1, 3, 1]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = ['i really love my dog',\n",
    "             'my dog loves my manatee']\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "test_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'my': 2,\n",
       " 'love': 3,\n",
       " 'dog': 4,\n",
       " 'i': 5,\n",
       " 'you': 6,\n",
       " 'cat': 7,\n",
       " 'do': 8,\n",
       " 'think': 9,\n",
       " 'is': 10,\n",
       " 'amazing': 11}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentence)\n",
    "word_index = tokenizer.word_index\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "test_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  4,  2,  1,  3],\n",
       "       [ 0,  0,  0,  4,  2,  1,  6],\n",
       "       [ 0,  0,  0,  5,  2,  1,  3],\n",
       "       [ 7,  5,  8,  1,  3,  9, 10]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded = pad_sequences(sequences)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  2,  1,  3,  0,  0,  0],\n",
       "       [ 4,  2,  1,  6,  0,  0,  0],\n",
       "       [ 5,  2,  1,  3,  0,  0,  0],\n",
       "       [ 7,  5,  8,  1,  3,  9, 10]], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, padding='post')\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  2,  1,  3,  0],\n",
       "       [ 4,  2,  1,  6,  0],\n",
       "       [ 5,  2,  1,  3,  0],\n",
       "       [ 8,  1,  3,  9, 10]], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, padding='post', maxlen=5)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 2, 1, 3, 0],\n",
       "       [4, 2, 1, 6, 0],\n",
       "       [5, 2, 1, 3, 0],\n",
       "       [7, 5, 8, 1, 3]], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, padding='post', truncating='post', maxlen=5)\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Training a model to recognize sentiment in text </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONDecodeError: Extra data: line 2 column 1 (char 217)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "try:\n",
    "    with open('Sarcasm_Headlines_Dataset.json','r') as f:\n",
    "        datastore = json.load(f)\n",
    "\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    urls = []\n",
    "    for i in datastore:\n",
    "        sentences.append(i['headline'])\n",
    "        labels.append(i['is_sarcastic'])\n",
    "        urls.append(i['article_link'])\n",
    "except:\n",
    "    print(\"JSONDecodeError: Extra data: line 2 column 1 (char 217)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2  https://local.theonion.com/mom-starting-to-fea...   \n",
       "3  https://politics.theonion.com/boehner-just-wan...   \n",
       "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "\n",
       "                                            headline  is_sarcastic  \n",
       "0  former versace store clerk sues over secret 'b...             0  \n",
       "1  the 'roseanne' revival catches up to our thorn...             0  \n",
       "2  mom starting to fear son's web series closest ...             1  \n",
       "3  boehner just wants wife to listen, not come up...             1  \n",
       "4  j.k. rowling wishes snape happy birthday in th...             0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"./Sarcasm_Headlines_Dataset.json\", lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(df['article_link'])\n",
    "labels = list(df['is_sarcastic'])\n",
    "urls = list(df['article_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    2     4     5     3     6 12731    95  2105     8 12732     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0]\n",
      "(26709, 46)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequences, padding='post')\n",
    "\n",
    "print(padded[0])\n",
    "print(padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = 20000\n",
    "training_sentences = sentences[0:training_size]\n",
    "testing_sentences = sentences[training_size:]\n",
    "training_labels = labels[0:training_size]\n",
    "testing_labels = labels[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 100\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "training_size = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    2     4     5     3     6 12731    95  2105     8 12732     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0]\n",
      "(26709, 46)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "print(padded[0])\n",
    "print(padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 24)                408       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 160,433\n",
      "Trainable params: 160,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "training_padded = np.array(training_padded)\n",
    "training_labels = np.array(training_labels)\n",
    "testing_padded = np.array(testing_padded)\n",
    "testing_labels = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 - 1s - loss: 0.4381 - accuracy: 0.8069 - val_loss: 0.0698 - val_accuracy: 0.9999\n",
      "Epoch 2/5\n",
      "625/625 - 1s - loss: 0.0224 - accuracy: 1.0000 - val_loss: 0.0070 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "625/625 - 1s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "625/625 - 1s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "625/625 - 1s - loss: 5.5410e-04 - accuracy: 1.0000 - val_loss: 6.0505e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9967818]\n",
      " [0.970959 ]]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"granny starting to fear spiders in the garden might be real\", \"game of thrones season finale showing this sunday night\"]\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "print(model.predict(padded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Spec\n",
    "\n",
    "\n",
    "### Tokenizer\n",
    "\n",
    "- Tensorflow의 text 전처리에 있음\n",
    "\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text\n",
    "\n",
    "\n",
    "- Tokenizer: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "\n",
    "- text to word sequence: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/text_to_word_sequence\n",
    "        \n",
    "\n",
    "### Embedding\n",
    "\n",
    "- Tensorflow의 Layer에 있음.\n",
    "\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
    "\n",
    "\n",
    "- Embedding : https://www.tensorflow.org/tutorials/text/word_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> 4. Machine Learning with Recurrent Neural Networks </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
